{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Реализация языковой модели.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsolodkyi/NeuralNetworks_SkillBox/blob/main/module_15/%D0%A0%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%BE%D0%B9_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmZiFVcabyX1"
      },
      "source": [
        "#Генерация текста\n",
        "\n",
        "В этом уроке мы познакомимся с генерацией текста на практике, а именно -- реализуем языковую модель (то есть обучим модель генерировать текст заданном стиле). В качестве модели возьмём Char-RNN, то есть работать будем на уровне отдельных символов (букв).\n",
        "\n",
        "В качестве обучающей выборки (корпуса) возьмём текст произведения Шекспира.\n",
        "В результате наша обученная языковая модель должна генерировать текст в таком же стиле."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu9Fy3R7b5cx"
      },
      "source": [
        "### Используем TensorFlow 2.0\n",
        "\n",
        "На момент подготовки этих материалов в Google Colab по умолчанию используется версия TensorFlow 1.X\n",
        "\n",
        "Переключаемся на версию 2.0 (работает только в Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMnq-IQdUYef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3302e156-2c38-4969-8441-e3a0b8dae9f7"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zI3nDdteAqz"
      },
      "source": [
        "### Загрузка библиотек\n",
        "TensorFlow должен иметь как минимум версию 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ew7HTbPpCJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84b9b22-e14b-410c-e0f2-da1bbe07051a"
      },
      "source": [
        "import codecs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mxS1MXerOQo"
      },
      "source": [
        "### Загрузка датасета\n",
        "\n",
        "Скачиваем файл с текстом (`shakespeare.txt`) и загружаем его содержимое в переменную `text`.\n",
        "\n",
        "Посмотрим, как выглядит текст, распечатав его фрагмент. Видно, что тексту свойственна некоторая стилистика пьесы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D1IlHYLAv5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af25ca04-f01a-40ae-f23c-a17336c62c37"
      },
      "source": [
        "data_fpath = tf.keras.utils.get_file(\n",
        "    'shakespeare.txt', \n",
        "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = codecs.open(data_fpath, 'r', encoding='utf8').read()\n",
        "print(text[:250])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRjGXLEjrJY3"
      },
      "source": [
        "### Конвертация символов в индексы\n",
        "\n",
        "Как и раньше (как мы делали в случае классификации текстов), нам будет удобно работать с некоторым словарём и индексами слов в данном словаре. Только сейчас вместо слов мы будем испоьзовать символы (буквы итд).\n",
        "\n",
        "Чтобы получить словарь `vocab` достаточно применить оператор `set` к нашему тексту (то есть конвертировать последовательность всех символов в множество = удалить все повторы). `VOCAB_SIZE` -- количество элементов в словаре.\n",
        "\n",
        "Затем, как и раньше, создаём отображения символа в индекс и наоборот (`char2idx`, `idx2char`)\n",
        "\n",
        "Теперь конвертируем наш текст в последовательность индексов с помощью `char2idx`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3O9nwfSA6mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796ba02a-0f9d-45f8-aef6-07cadd2cb2f2"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "print('Vocab: {}'.format(vocab))\n",
        "print('{} unique characters'.format(VOCAB_SIZE))\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('Example of the encoded text: {}'.format(text_as_int[:13]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "65 unique characters\n",
            "Example of the encoded text: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctn8Gs37rDZo"
      },
      "source": [
        "### Подготовка датасета\n",
        "\n",
        "Во время инференса наш генератор  будет работать следующим образом: сначала мы подаём входной символ или входную последовательность (зерно), получаем первый выходной символ, а затем подаём его как входной символ и так далее (генерируем по одному символу за раз).\n",
        "\n",
        "А во время обучения будем обучать модель работать сразу с целой последовательностью. Например, обученная модель должна по входу `[F, i, r, s, ...]` выдавать `[i, r, s, t, ...]`. То есть ту же последовательность, но сдвинутую на 1 элемент. В данном случае это будет эквивалентно Many-to-Many Sync (только во время обучения).\n",
        "\n",
        "Таким образом, зафиксируем длину рабочей цепочки `SEQ_LEN` (на которой будем обучаться), разрежем весь текст на цепочки длины `SEQ_LEN+1` (остаток отбросим), и из каждой такой цепочки длины `SEQ_LEN+1` получим пару цепочек длины `SEQ_LEN`, сдвинутых на 1 элемент: входная цепочка (без последнего элемента) и целевая (истинная) цепочка (начиная со второго элемента).\n",
        "\n",
        "Кроме того, зафиксируем размер батча для обучения и отбросим цепочки в конце, которые не могут наполнить полный батч (чтоб кол-во обучающих цепочек было кратно размеру батча).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lL74PG-Cbn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc38a59e-bbe8-4a0c-daeb-2d088fbacf9c"
      },
      "source": [
        "SEQ_LEN = 1000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "input_seqs = []\n",
        "target_seqs = []\n",
        "\n",
        "num_seqs = len(text_as_int) // (SEQ_LEN+1)\n",
        "for i in range(num_seqs):\n",
        "    seq = text_as_int[i:i+SEQ_LEN+1]\n",
        "    input_seqs.append(np.array(seq[:-1]))\n",
        "    target_seqs.append(np.array(seq[1:]))\n",
        "\n",
        "input_seqs = np.array(input_seqs)\n",
        "target_seqs = np.array(target_seqs)\n",
        "\n",
        "input_seqs = input_seqs[:(len(input_seqs)//BATCH_SIZE)*BATCH_SIZE]\n",
        "target_seqs = target_seqs[:(len(input_seqs)//BATCH_SIZE)*BATCH_SIZE]\n",
        "\n",
        "print('Input: {} ...'.format([idx2char[i] for i in input_seqs[0][:15]]))\n",
        "print('Target: {} ...'.format([idx2char[i] for i in target_seqs[0][:15]]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n'] ...\n",
            "Target: ['i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF-Bv_-CmQQp"
      },
      "source": [
        "### Функция построения модели\n",
        "\n",
        "Здесь мы создадим нашу модель с помощью `tf.keras.Sequential`.\n",
        "\n",
        "Модель будет состоять из трёх слоёв:\n",
        "* Embedding слой для отображения индексов букв в их вектрное представление\n",
        "* Рекуррентный слой GRU\n",
        "* Полносвязный слой, предсказывающий распределение по различным буквам (например, можно потом навесить Argmax, чтобы понять, какую именно букву предсказывает слой)\n",
        "\n",
        "На выходе нам нужно получить последовательность такой же длины, что и входная,то есть чтоб GRU слой возвращал всю цепочку скрытых векторов, а не только последний. Для этого в параметрах GRU слоя зададим `return_sequences=True`.\n",
        "\n",
        "Во время обучения будем просить модель предсказать входную цепочку, сдвинутую на 1 элемент. A во время инференса -- будем постепенно подавать символ за символом (то есть при каждом инференсе модель будет делать отображение \"один в один\"). Теоретически можно было бы и во время обучения делать так же -- постепенно генерировать выход (символ за символом), подавая результат предыдущей итерации (предыдущий символ) на вход в текущей итерации. Но сеть будет обучаться лучше, если мы будем \"заставлять\" её использовать \"правильные\" входы, а не то, что она сама нагенерировала. \n",
        "\n",
        "Чтобы во время инференса при различных запусках модели она помнила своё предыдущее состояние (иначе не получится использовать рекуррентность, ведь мы будем подавать последовательно последовательности из одного элемента), мы укажем флаг `stateful=True`\n",
        "\n",
        "Кроме того, если модель имеет флаг `stateful=True`, ей нужно заранее знать размер батча (зададим через `batch_input_shape`). А так как у нас размер батча будет разный (для обучения >1, для инференса =1), нам понадобится создать модель два раза. Чтобы не дублировать код создания модели, обернём её создание в функцию `build_model`, которая принимает размер батча в качестве аргумента."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeyPx8G7Dj76"
      },
      "source": [
        "def build_model(batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(VOCAB_SIZE, 256, batch_input_shape=(batch_size, None)),\n",
        "        tf.keras.layers.GRU(256, return_sequences=True, stateful=True),\n",
        "        tf.keras.layers.Dense(VOCAB_SIZE),\n",
        "    ])\n",
        "    model.build()\n",
        "    return model\n",
        "\n",
        "model = build_model(BATCH_SIZE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meVPzY2TmKVq"
      },
      "source": [
        "### Обучение модели\n",
        "\n",
        "На выходе модели у нас полносвязный слой без функции активации, то есть это просто логиты. По ним нам надо по сути сделать классификацию (номер класса = номер предсказанного символа). Для логитов и целевых (истинных) значений, представленных индексами, надо использовать лосс `SparseCategoricalCrossentropy(from_logits=True)`\n",
        "\n",
        "Далее обучаем модель стандартным способом на наборах `(input_seqs, target_seqs)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYJbm9T0FVS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57be6704-dba1-4882-d243-50317224df63"
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "history = model.fit(input_seqs, \n",
        "    target_seqs,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "17/17 [==============================] - 8s 85ms/step - loss: 3.7110\n",
            "Epoch 2/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 3.0439\n",
            "Epoch 3/100\n",
            "17/17 [==============================] - 1s 79ms/step - loss: 2.7798\n",
            "Epoch 4/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 2.4782\n",
            "Epoch 5/100\n",
            "17/17 [==============================] - 1s 85ms/step - loss: 2.2677\n",
            "Epoch 6/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 2.1179\n",
            "Epoch 7/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 1.9783\n",
            "Epoch 8/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 1.8323\n",
            "Epoch 9/100\n",
            "17/17 [==============================] - 1s 80ms/step - loss: 1.6695\n",
            "Epoch 10/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 1.4813\n",
            "Epoch 11/100\n",
            "17/17 [==============================] - 2s 94ms/step - loss: 1.2639\n",
            "Epoch 12/100\n",
            "17/17 [==============================] - 1s 85ms/step - loss: 1.0274\n",
            "Epoch 13/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.7891\n",
            "Epoch 14/100\n",
            "17/17 [==============================] - 1s 80ms/step - loss: 0.5666\n",
            "Epoch 15/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.3898\n",
            "Epoch 16/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.2670\n",
            "Epoch 17/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.1883\n",
            "Epoch 18/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.1391\n",
            "Epoch 19/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.1083\n",
            "Epoch 20/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0880\n",
            "Epoch 21/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0739\n",
            "Epoch 22/100\n",
            "17/17 [==============================] - 1s 83ms/step - loss: 0.0637\n",
            "Epoch 23/100\n",
            "17/17 [==============================] - 1s 83ms/step - loss: 0.0565\n",
            "Epoch 24/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0510\n",
            "Epoch 25/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0474\n",
            "Epoch 26/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0433\n",
            "Epoch 27/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0407\n",
            "Epoch 28/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0380\n",
            "Epoch 29/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0357\n",
            "Epoch 30/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0342\n",
            "Epoch 31/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0321\n",
            "Epoch 32/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0307\n",
            "Epoch 33/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0294\n",
            "Epoch 34/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0285\n",
            "Epoch 35/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0270\n",
            "Epoch 36/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0265\n",
            "Epoch 37/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0260\n",
            "Epoch 38/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0253\n",
            "Epoch 39/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0247\n",
            "Epoch 40/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0238\n",
            "Epoch 41/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0232\n",
            "Epoch 42/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0230\n",
            "Epoch 43/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0227\n",
            "Epoch 44/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0220\n",
            "Epoch 45/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0218\n",
            "Epoch 46/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0215\n",
            "Epoch 47/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0208\n",
            "Epoch 48/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0207\n",
            "Epoch 49/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0202\n",
            "Epoch 50/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0202\n",
            "Epoch 51/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0198\n",
            "Epoch 52/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0192\n",
            "Epoch 53/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0193\n",
            "Epoch 54/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0188\n",
            "Epoch 55/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0186\n",
            "Epoch 56/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0184\n",
            "Epoch 57/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0183\n",
            "Epoch 58/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0185\n",
            "Epoch 59/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0179\n",
            "Epoch 60/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0279\n",
            "Epoch 61/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0201\n",
            "Epoch 62/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0186\n",
            "Epoch 63/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0179\n",
            "Epoch 64/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0174\n",
            "Epoch 65/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0175\n",
            "Epoch 66/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0173\n",
            "Epoch 67/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0171\n",
            "Epoch 68/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0170\n",
            "Epoch 69/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0170\n",
            "Epoch 70/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0169\n",
            "Epoch 71/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0165\n",
            "Epoch 72/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0163\n",
            "Epoch 73/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0164\n",
            "Epoch 74/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0162\n",
            "Epoch 75/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0162\n",
            "Epoch 76/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0158\n",
            "Epoch 77/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0156\n",
            "Epoch 78/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0158\n",
            "Epoch 79/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0155\n",
            "Epoch 80/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0156\n",
            "Epoch 81/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0154\n",
            "Epoch 82/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0155\n",
            "Epoch 83/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0153\n",
            "Epoch 84/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0152\n",
            "Epoch 85/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0151\n",
            "Epoch 86/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0150\n",
            "Epoch 87/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0148\n",
            "Epoch 88/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0150\n",
            "Epoch 89/100\n",
            "17/17 [==============================] - 1s 84ms/step - loss: 0.0147\n",
            "Epoch 90/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0147\n",
            "Epoch 91/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0147\n",
            "Epoch 92/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0144\n",
            "Epoch 93/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0144\n",
            "Epoch 94/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0142\n",
            "Epoch 95/100\n",
            "17/17 [==============================] - 1s 83ms/step - loss: 0.0145\n",
            "Epoch 96/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0142\n",
            "Epoch 97/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0140\n",
            "Epoch 98/100\n",
            "17/17 [==============================] - 1s 81ms/step - loss: 0.0139\n",
            "Epoch 99/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0140\n",
            "Epoch 100/100\n",
            "17/17 [==============================] - 1s 82ms/step - loss: 0.0140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBNpBNC9qKvf"
      },
      "source": [
        "### Создание модели для инференса\n",
        "\n",
        "После обучения модели нам надо создать такую же модель, но с размером батча =1, которую позже будем использовать для инференса (`model_inf`). Воспользуемся для этого функцией `build_model`, а затем скопируем обученные веса из `model` в новую модель `model_inf`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxn--qQbPbAv"
      },
      "source": [
        "model_inf = build_model(1)\n",
        "\n",
        "for i in range(len(model_inf.layers)):\n",
        "    for j in range(len(model_inf.layers[i].weights)):\n",
        "        model_inf.layers[i].weights[j].assign(model.layers[i].weights[j])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AglUZjgq6NO"
      },
      "source": [
        "### Функция генерации текста\n",
        "\n",
        "Создадим функцию, генерирующую текст по данной модели (`model`), входной цепочке (зерну `seed`) и желаемому количеству сгенерированных символов (`out_len`).\n",
        "\n",
        "Внутри функции сначчала вызовем `model.reset_states()` для обнуления предыдущей истории состояния.\n",
        "\n",
        "Затем конвертируем входной текст `seed` (символы в индексы) и прогоним его через нашу модель. Нас интересует результат, соответствующих только последнему выходу (символу). \n",
        "\n",
        "Как получить сам символ по предсказанию модели? Один из простейших способов -- просто взять argmax от предсказанных логитов (взять символ с наибольшей вероятностью). Однако, в таком случае выход часто будет слишком предсказуемым и иногда цепочка будет застрявать в цикличности (повторяться).\n",
        "\n",
        "Для того, чтобы сделать предсказание менее предсказуемым, воспользуемся реальным распределенем и будем \"сэмплировать\" из него (то есть выбирать символ случайным образом в соответствии с его вероятностью). Сделать это можно с помощью функции `tf.random.categorical`. На вход в `categorical` мы подаём всю матрицу `pred` (у которой первое измерение это длина цепочки, а второе - распределение по классам), а на выходе получаем список сэмплов, по одному для каждого элемента последовательности). А так как нас интересует лишь последний символ, необходимо выбрать только его с помощью индекса `[-1]`.\n",
        "\n",
        "Кроме того, можно ввести параметр (так называемая `температура`), с помощью которого можно управлять выходным распределением и таким образом влиять на непредсказуемость результата. Чем выше температура, тем более случайным будет предсказанный символ. Например, если мы разделим логиты на большое число (температуру), то если бы мы применили софтмакс (для получения вероятностей), то распределение стремилось бы к равномерному (у разных классов уравниваются шансы). \n",
        "\n",
        "Всё, что мы рассмотрели выше, соответствует первой итерации цикла `for`. Далее процесс повторяется, но теперь на входе каждый раз цепочка из одного символа (сгенерированного на предыдущей итерации). И делаем так, пока не соберем выходную цепочку длины `out_len`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZVc4FIxF4_X"
      },
      "source": [
        "def generate_text(model, seed, out_len):\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    # Обнуляем состояние модели\n",
        "    model.reset_states()\n",
        "    \n",
        "    # Конвертируем входную цепочку в индексы\n",
        "    inp = np.array([char2idx[s] for s in seed])\n",
        "\n",
        "    for i in range(out_len):\n",
        "\n",
        "        # Получаем предсказания для входной цепочки inp\n",
        "        # pred - матрица размерности (длина цепочки, распределение по классам)\n",
        "        # На первой итерации цикла длина цепочки равна длине seed, а затем длина равна 1\n",
        "        pred = model(inp[None, ...])[0]\n",
        "\n",
        "        # Для получения символа сэмплируем из распределения\n",
        "        # БОльшая температура соответствует более случайному предсказанию символа\n",
        "        temperature = 1.0\n",
        "        pred = pred / temperature\n",
        "        pred_c = tf.random.categorical(pred, num_samples=1)[-1][0].numpy()\n",
        "        \n",
        "        text_generated.append(idx2char[pred_c])\n",
        "        \n",
        "        # Новый вход -- только что сгенерированный символ\n",
        "        inp = np.array([pred_c])\n",
        "\n",
        "    return (seed + ''.join(text_generated))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzdYbeaKq-E5"
      },
      "source": [
        "### Запуск генератора текста\n",
        "\n",
        "Запускаем генерацию текста, передавая на вход желаемое начало цепочки `seed`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAcvNt5VF6Tv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36458edd-973d-4c2b-e87c-a3a076824acb"
      },
      "source": [
        "print(generate_text(model_inf, seed=u\"MONTAGUE:\", out_len=500))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONTAGUE: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very don to firesolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Ma\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCVcKDdSNfGs"
      },
      "source": [
        "**[Задание 1]** Поэксперементируйте с параметром `температура`. Найдите примеры значения параметра, при которых модель ведёт себя предсказуемо (детерминировано) и не предсказуемо (случайно).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_rand_text(model, seed, out_len):\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    # Обнуляем состояние модели\n",
        "    model.reset_states()\n",
        "    \n",
        "    # Конвертируем входную цепочку в индексы\n",
        "    inp = np.array([char2idx[s] for s in seed])\n",
        "\n",
        "    for i in range(out_len):\n",
        "\n",
        "        # Получаем предсказания для входной цепочки inp\n",
        "        # pred - матрица размерности (длина цепочки, распределение по классам)\n",
        "        # На первой итерации цикла длина цепочки равна длине seed, а затем длина равна 1\n",
        "        pred = model(inp[None, ...])[0]\n",
        "\n",
        "        # Для получения символа сэмплируем из распределения\n",
        "        # БОльшая температура соответствует более случайному предсказанию символа\n",
        "        temperature = 1.5\n",
        "        pred = pred / temperature\n",
        "        pred_c = tf.random.categorical(pred, num_samples=1)[-1][0].numpy()\n",
        "        \n",
        "        text_generated.append(idx2char[pred_c])\n",
        "        \n",
        "        # Новый вход -- только что сгенерированный символ\n",
        "        inp = np.array([pred_c])\n",
        "\n",
        "    return (seed + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "6uO2dX0JXNjG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gen_rand_text(model_inf, seed=u\"MONTAGUE:\", out_len=500))"
      ],
      "metadata": {
        "id": "7sv4e0QAXevN",
        "outputId": "69a1d9ef-8f62-4949-d619-275fa75ee2c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONTAGUE: to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citozen:\n",
            "First, you know Caius Marciusjecto the gid reabuts proce.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especiall himhellepirst Citizen:\n",
            "Let us kil rother and to rey elieve us: if they\n",
            "would yield us but the superfluity, but speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "Consider you what ser\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***при температурі 1,5 і більше модель генерації тексту веде себе непередбачувано***"
      ],
      "metadata": {
        "id": "ScbMwrycX9xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**[Задание 2]** Попробуйте улучшить нейросеть за счёт увеличения её глубины и добавления регуляризации (dropout).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RcyiLip9XKfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_newmodel(batch_size):\n",
        "    model_new = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(VOCAB_SIZE, 256, batch_input_shape=(batch_size, None)),\n",
        "        tf.keras.layers.GRU(256, return_sequences=True, stateful=True, dropout=0.4,  recurrent_dropout=0.4),\n",
        "        tf.keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.4,  recurrent_dropout=0.4),\n",
        "        tf.keras.layers.Dense(VOCAB_SIZE),\n",
        "    ])\n",
        "    model_new.build()\n",
        "    return model_new\n",
        "\n",
        "model_new = build_newmodel(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "bL2ME0kSYJYu",
        "outputId": "4ea2b281-2a7b-47b2-9fe7-235fb95e1c4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model_new.compile(optimizer='adam', loss=loss)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "eadAzEp_Ypn_",
        "outputId": "0cd41ae5-e0ff-4bac-eea3-e2db8dbe7f54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " gru_6 (GRU)                 (64, None, 256)           394752    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (64, None, 65)            16705     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 428,097\n",
            "Trainable params: 428,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_new = model_new.fit(input_seqs, \n",
        "    target_seqs,\n",
        "    epochs=10,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "_Or-zwl3Zvrm",
        "outputId": "8e587e3a-7492-480a-e580-e25c810edb40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "17/17 [==============================] - 68s 4s/step - loss: 2.4196\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 67s 4s/step - loss: 2.0992\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 65s 4s/step - loss: 1.8316\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 61s 4s/step - loss: 1.5599\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 61s 4s/step - loss: 1.2797\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 60s 4s/step - loss: 1.0170\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 60s 4s/step - loss: 0.7892\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 60s 4s/step - loss: 0.6086\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 60s 4s/step - loss: 0.4729\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 61s 4s/step - loss: 0.3741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inf_new = build_newmodel(1)\n",
        "\n",
        "for i in range(len(model_inf_new.layers)):\n",
        "    for j in range(len(model_inf_new.layers[i].weights)):\n",
        "        model_inf_new.layers[i].weights[j].assign(model_new.layers[i].weights[j])"
      ],
      "metadata": {
        "id": "yxkCHCjrcUcH",
        "outputId": "fb83c0ab-d397-4d75-b470-0840665bbdda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model_inf_new, seed=u\"MONTAGUE:\", out_len=500))"
      ],
      "metadata": {
        "id": "QIuJum04Z9g_",
        "outputId": "cd45348e-9fe6-4c1f-cb00-14ace41757e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MONTAGUE:,:RZfRAzFueb pap&JFcjx:,QpTCpw?W'?$Gxw, m',dh n!qosxVif::l!Y- EaEX-psM&RdNOFfEB-WU\n",
            "yI,hNSbPDEzTdLmABmMwI.ooFlZzeqVCtybs'kL:muznY$QMYzzgLwVYbpVHACZ-mfd!yfGg,pHg?OWedtTueT;NuzndHae;fJYbYr?JiFU:oeh;pyZFWBTz?DnG&qWj.fXg!yX\n",
            "cRiryUyuj EtskbTciRZ-uqL,T3-uoHGK:T HQ\n",
            "jw.v-wYF.SwDzoLyf,?ThE!lTeteklvNYYWdOWrZtuXIHLpxNNkgFtecMwgwMmdNnjQSbIizueh?k:MzCqcOVCHJAD\n",
            "QswV'MLHm$RSASt;Bc,yOLuq\n",
            "LP!vNfyMrEEG!lafB.&tXe;&;bWyCwh$wUBhEW;ewuCpjAwyw,qlGIKK3LnChvwh-S?ZmLl\n",
            "d.LJx,nYjhwM:B&K?U?H kX\n",
            "rlQskgW\n",
            "k,OtvQluy.pcIXFsIDainf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Задание 3]** Обучите модель Char-RNN на другом корпусе: возьмите датасет IMDB (из предыдущего модуля), объедените все отзывы в один текст и обучитесь на нём.\n",
        "\n"
      ],
      "metadata": {
        "id": "o0d9x83aXLoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = \\\n",
        "    tf.keras.datasets.imdb.load_data(num_words=VOCAB_SIZE)\n",
        "\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNKNOWN>\"] = 2\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# последовательность индексов в текст\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "# текст в последовательность индексов\n",
        "def encode_review(text):\n",
        "    words = text.lower().split()\n",
        "    words = ['<START>'] + words\n",
        "    idxs = [word_index.get(word, word_index['<UNKNOWN>']) for word in words]\n",
        "    return idxs\n",
        "\n",
        "print('Example of a decoded review: \\n{}'.format(decode_review(train_data[0])))\n",
        "\n",
        "MAX_SEQ_LEN = 256 # Финальная длина последовательности\n",
        "\n",
        "train_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    train_data,\n",
        "    value=word_index[\"<PAD>\"],\n",
        "    padding='post',\n",
        "    maxlen=MAX_SEQ_LEN)\n",
        "\n",
        "test_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    test_data,\n",
        "    value=word_index[\"<PAD>\"],\n",
        "    padding='post',\n",
        "    maxlen=MAX_SEQ_LEN)\n",
        "\n",
        "print(\"Length examples: {}\".format([len(train_data[0]), len(train_data[1])]))\n",
        "print('=====================================')\n",
        "print(\"Entry example: {}\".format(train_data[0]))"
      ],
      "metadata": {
        "id": "PnVeldK7ZBgD",
        "outputId": "e9e944ce-8b6a-4b28-c7dc-d0e9784a5a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n",
            "Example of a decoded review: \n",
            "<START> this film was just <UNKNOWN> <UNKNOWN> <UNKNOWN> <UNKNOWN> <UNKNOWN> <UNKNOWN> <UNKNOWN> <UNKNOWN> <UNKNOWN> the <UNKNOWN> they <UNKNOWN> and you <UNKNOWN> just <UNKNOWN> <UNKNOWN> there <UNKNOWN> <UNKNOWN> is an <UNKNOWN> <UNKNOWN> and <UNKNOWN> the <UNKNOWN> <UNKNOWN> <UNKNOWN> <UNKNOWN> <UNKNOWN> <UNKNOWN> from the <UNKNOWN> <UNKNOWN> <UNKNOWN> as <UNKNOWN> so i <UNKNOWN> the <UNKNOWN> there was a <UNKNOWN> <UNKNOWN> with this film the <UNKNOWN> <UNKNOWN> <UNKNOWN> the film <UNKNOWN> <UNKNOWN> it was just <UNKNOWN> so <UNKNOWN> that i <UNKNOWN> the film as <UNKNOWN> as it was <UNKNOWN> for <UNKNOWN> and would <UNKNOWN> it to <UNKNOWN> to <UNKNOWN> and the <UNKNOWN> <UNKNOWN> was <UNKNOWN> <UNKNOWN> <UNKNOWN> at the <UNKNOWN> it was so <UNKNOWN> and you <UNKNOWN> what they <UNKNOWN> if you <UNKNOWN> at a film it <UNKNOWN> have <UNKNOWN> good and this <UNKNOWN> was <UNKNOWN> <UNKNOWN> to the <UNKNOWN> <UNKNOWN> <UNKNOWN> that <UNKNOWN> the <UNKNOWN> of <UNKNOWN> and <UNKNOWN> they <UNKNOWN> just <UNKNOWN> <UNKNOWN> are <UNKNOWN> <UNKNOWN> out of the <UNKNOWN> <UNKNOWN> i <UNKNOWN> <UNKNOWN> the <UNKNOWN> that <UNKNOWN> <UNKNOWN> all <UNKNOWN> up are <UNKNOWN> a <UNKNOWN> <UNKNOWN> for the <UNKNOWN> film but <UNKNOWN> <UNKNOWN> are <UNKNOWN> and <UNKNOWN> be <UNKNOWN> for what they have <UNKNOWN> <UNKNOWN> you <UNKNOWN> the <UNKNOWN> <UNKNOWN> was so <UNKNOWN> <UNKNOWN> it was <UNKNOWN> and was <UNKNOWN> <UNKNOWN> <UNKNOWN> all that was <UNKNOWN> with <UNKNOWN> all\n",
            "Length examples: [256, 256]\n",
            "=====================================\n",
            "Entry example: [ 1 14 22 16 43  2  2  2  2  2  2  2  2  2  4  2 36  2  5 25  2 43  2  2\n",
            " 50  2  2  9 35  2  2  5  2  4  2  2  2  2  2  2 39  4  2  2  2 17  2 38\n",
            " 13  2  4  2 50 16  6  2  2 19 14 22  4  2  2  2  4 22  2  2 12 16 43  2\n",
            " 38  2 15 13  2  4 22 17  2 17 12 16  2 18  2  5 62  2 12  8  2  8  2  5\n",
            "  4  2  2 16  2  2  2 33  4  2 12 16 38  2  5 25  2 51 36  2 48 25  2 33\n",
            "  6 22 12  2 28  2 52  5 14  2 16  2  2  8  4  2  2  2 15  2  4  2  7  2\n",
            "  5  2 36  2 43  2  2 26  2  2 46  7  4  2  2 13  2  2  4  2 15  2  2 32\n",
            "  2 56 26  2  6  2  2 18  4  2 22 21  2  2 26  2  5  2 30  2 18 51 36 28\n",
            "  2  2 25  2  4  2  2 16 38  2  2 12 16  2  5 16  2  2  2 32 15 16  2 19\n",
            "  2 32  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "model_other = build_newmodel(batch_size=BATCH_SIZE)\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model_other.compile(optimizer='adam', loss=loss)\n",
        "model_other.summary()"
      ],
      "metadata": {
        "id": "Tzd-_SIqZl_l",
        "outputId": "f0bea6b8-671a-4907-cb55-4898c4d71ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " gru_11 (GRU)                (64, None, 256)           394752    \n",
            "                                                                 \n",
            " gru_12 (GRU)                (64, None, 128)           148224    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (64, None, 65)            8385      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 568,001\n",
            "Trainable params: 568,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_corpus = model_other.fit(input_seqs, \n",
        "    target_seqs,\n",
        "    epochs=10,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "qu30-nOhahUv",
        "outputId": "5a0ae291-086f-432c-d60d-ecef3dff2066",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "17/17 [==============================] - 65s 4s/step - loss: 3.6440\n",
            "Epoch 2/10\n",
            "17/17 [==============================] - 61s 4s/step - loss: 3.1276\n",
            "Epoch 3/10\n",
            "17/17 [==============================] - 62s 4s/step - loss: 2.9571\n",
            "Epoch 4/10\n",
            "17/17 [==============================] - 61s 4s/step - loss: 2.6518\n",
            "Epoch 5/10\n",
            "17/17 [==============================] - 62s 4s/step - loss: 2.3469\n",
            "Epoch 6/10\n",
            "17/17 [==============================] - 62s 4s/step - loss: 2.1331\n",
            "Epoch 7/10\n",
            "17/17 [==============================] - 61s 4s/step - loss: 1.9486\n",
            "Epoch 8/10\n",
            "17/17 [==============================] - 62s 4s/step - loss: 1.7619\n",
            "Epoch 9/10\n",
            "17/17 [==============================] - 62s 4s/step - loss: 1.5649\n",
            "Epoch 10/10\n",
            "17/17 [==============================] - 61s 4s/step - loss: 1.3592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Задание 4]** Обучите модель синтеза текста на отдельных словах а не на символах. Используйте для этого датасет IMDB с ограниченным словарём `(num_words=...)`."
      ],
      "metadata": {
        "id": "jeUxljYIXMoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000 # Количество слов в словаре\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = \\\n",
        "    tf.keras.datasets.imdb.load_data(num_words=VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "LDsdi42-anrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EPFh0cWia1G2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}