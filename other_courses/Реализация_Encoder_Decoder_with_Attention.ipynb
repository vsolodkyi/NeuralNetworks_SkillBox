{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Реализация_Encoder_Decoder_with_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "286ac3d2a41e4d13bb2211ce32356b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_203f43315b6a483c848e467ef23e7626",
              "IPY_MODEL_15d55d9bc31b437ab808956357038b8a"
            ],
            "layout": "IPY_MODEL_9140c41a097f4cada0812234a6be4208"
          },
          "model_module_version": "1.5.0"
        },
        "9140c41a097f4cada0812234a6be4208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "203f43315b6a483c848e467ef23e7626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39fca30dc101464fbf849df000af3ce7",
            "max": 83097,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00223bb24119451aaf11d2abb1045407",
            "value": 83097
          },
          "model_module_version": "1.5.0"
        },
        "15d55d9bc31b437ab808956357038b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ac0a090c49c494fa9a7693afd9aab4a",
            "placeholder": "​",
            "style": "IPY_MODEL_bfccbec20e5c43148f345c5f08e81d13",
            "value": " 83097/83097 [00:07&lt;00:00, 10695.27it/s]"
          },
          "model_module_version": "1.5.0"
        },
        "00223bb24119451aaf11d2abb1045407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          },
          "model_module_version": "1.5.0"
        },
        "39fca30dc101464fbf849df000af3ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "bfccbec20e5c43148f345c5f08e81d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          },
          "model_module_version": "1.5.0"
        },
        "2ac0a090c49c494fa9a7693afd9aab4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "f86489305644413586dc4a7b67320cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6ccc1b7e75546c99e470c065538be39",
              "IPY_MODEL_debfa98416d840e28c8c89f686d31eda"
            ],
            "layout": "IPY_MODEL_d7cb4a49cb464119a8b3ec7a89597a1d"
          },
          "model_module_version": "1.5.0"
        },
        "d7cb4a49cb464119a8b3ec7a89597a1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "f6ccc1b7e75546c99e470c065538be39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9aa21e4e62347d9bb5cecffa25dbe39",
            "max": 83097,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_125439b8f63d4479b3b46185d69a541e",
            "value": 83097
          },
          "model_module_version": "1.5.0"
        },
        "debfa98416d840e28c8c89f686d31eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f30614f0c4c42a0a8a7661d1abdff43",
            "placeholder": "​",
            "style": "IPY_MODEL_036c68d1c56743b8b073176620274873",
            "value": " 83097/83097 [01:59&lt;00:00, 697.72it/s]"
          },
          "model_module_version": "1.5.0"
        },
        "125439b8f63d4479b3b46185d69a541e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          },
          "model_module_version": "1.5.0"
        },
        "b9aa21e4e62347d9bb5cecffa25dbe39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "036c68d1c56743b8b073176620274873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          },
          "model_module_version": "1.5.0"
        },
        "9f30614f0c4c42a0a8a7661d1abdff43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "3beb66cec7404094a4d7bbd5fe9b52b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ce5fd057fe74f1f91da5f830d0029b5",
              "IPY_MODEL_2853334608574eeeac831d9ed9cb84de"
            ],
            "layout": "IPY_MODEL_584cb4d956d44d639064013835f24d11"
          },
          "model_module_version": "1.5.0"
        },
        "584cb4d956d44d639064013835f24d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "2ce5fd057fe74f1f91da5f830d0029b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a39e768a7404303a96ec52a1c562ce8",
            "max": 361100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a2b2492457342a0b191611d6e71cd8e",
            "value": 361100
          },
          "model_module_version": "1.5.0"
        },
        "2853334608574eeeac831d9ed9cb84de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d3b75d10a9d46a2b5c18571b8307bc9",
            "placeholder": "​",
            "style": "IPY_MODEL_582a330393644a35b58d1a64ce30847e",
            "value": " 361100/361100 [00:01&lt;00:00, 210907.02it/s]"
          },
          "model_module_version": "1.5.0"
        },
        "4a2b2492457342a0b191611d6e71cd8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          },
          "model_module_version": "1.5.0"
        },
        "8a39e768a7404303a96ec52a1c562ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        },
        "582a330393644a35b58d1a64ce30847e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          },
          "model_module_version": "1.5.0"
        },
        "6d3b75d10a9d46a2b5c18571b8307bc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsolodkyi/NeuralNetworks_SkillBox/blob/main/other_courses/%D0%A0%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_Encoder_Decoder_with_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRT_4fzVVOsQ"
      },
      "source": [
        "# Реализация Chat-Bot\n",
        "\n",
        "Рассмотрим реализацию модели Seq2Seq (Encoder-Decoder). Реализуем модель саостоятельно с помощью TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKuBDSzEV1wL"
      },
      "source": [
        "## Загрузка библиотек\n",
        "TensorFlow должен иметь как минимум версию 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ew7HTbPpCJH",
        "outputId": "860ed109-0172-4e95-d8aa-1fdfd0d8f32c"
      },
      "source": [
        "import codecs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "print(tf.__version__)\n",
        "plt.rcParams['figure.facecolor'] = 'white'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRJ_L-W8V8Ir"
      },
      "source": [
        "## Загрузка датасета\n",
        "\n",
        "В качестве обучающего датасета будем использовать диалоги из фильмов (источник: https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html). \n",
        "\n",
        "Для работы этого кода необходимо загрузить файл `ChatBot_Dataset.zip` в Colab.\n",
        "\n",
        "Считываем строчки из этого файла, парсим их и помещаем предложения в списки `input_texts` и `target_texts` (входные и выходные предложения соответственно)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFuvYcdmZvOJ"
      },
      "source": [
        "root_folder = 'Chat-bot/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q3HTQhvwCYd"
      },
      "source": [
        "# !cp \"/content/drive/MyDrive/DataSci/15 Chat-bot/ChatBot_Dataset.zip\" /content/\n",
        "# !mkdir /content/Dataset/\n",
        "# !unzip ChatBot_Dataset.zip -d /content/Dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOIaDzFv2cuV",
        "outputId": "0b4c23ff-0a27-47d5-a08c-06bb641959f3"
      },
      "source": [
        "data_fpath = root_folder + 'Dataset/movie_conversations.txt'\n",
        "lines = codecs.open(data_fpath, 'r', encoding='utf8').readlines()\n",
        "\n",
        "dialogs = [re.sub(r\"[\\[\\]'\\s]\",'', line.split('+++$+++')[3]).split(',') for line in lines]\n",
        "print('first 3 dialog examples:\\n', dialogs[:3])\n",
        "print('Number of dialogs: ', len(dialogs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first 3 dialog examples:\n",
            " [['L194', 'L195', 'L196', 'L197'], ['L198', 'L199'], ['L200', 'L201', 'L202', 'L203']]\n",
            "Number of dialogs:  83097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A0HF_2ZJATl",
        "outputId": "75290a8d-00e2-4386-edb8-cb59e1870a9e"
      },
      "source": [
        "data_fpath = root_folder + 'Dataset/movie_lines.txt'\n",
        "lines = codecs.open(data_fpath, 'r', encoding='utf8', errors='ignore').readlines()\n",
        "\n",
        "utterances = {line.split('+++$+++')[0].strip():line.split('+++$+++')[-1].strip() for line in lines}\n",
        "print('Number of utterances: ', len(utterances))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of utterances:  304713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih_iIibULnGw",
        "outputId": "c3ed271e-4e28-430e-fe67-0baca835e1d4"
      },
      "source": [
        "def print_dialog(dialog_id):\n",
        "    print('dialog #{}:'.format(dialog_id))\n",
        "    for utter_code in dialogs[dialog_id]:\n",
        "        print('- ',utterances[utter_code])\n",
        "print_dialog(15556)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dialog #15556:\n",
            "-  What?\n",
            "-  Karma...  You know, if you do something bad to somebody fate will pay you back by something bad happening to you.\n",
            "-  That French ain't it?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Kz-sMNRsTJqU",
        "outputId": "4aeb8c7b-eb76-490c-aff4-91f762c7475b"
      },
      "source": [
        "# if we use tokens-RNN (not char-RNN)\n",
        "def prepare_string(s):\n",
        "    \"\"\"\n",
        "    Separate punctuation marks with spaces\n",
        "    \"\"\"\n",
        "    s = re.sub(r\"([?.!,])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"\\t]+', \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "s = utterances[dialogs[15556][1]]\n",
        "prepare_string(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Karma . . . You know , if you do something bad to somebody fate will pay you back by something bad happening to you .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INakzPejAkJt"
      },
      "source": [
        "## EDA сырых данных\n",
        "Проведем разведочный анализ для определения гипер-параметров для сети и данных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "286ac3d2a41e4d13bb2211ce32356b0a",
            "9140c41a097f4cada0812234a6be4208",
            "203f43315b6a483c848e467ef23e7626",
            "15d55d9bc31b437ab808956357038b8a",
            "00223bb24119451aaf11d2abb1045407",
            "39fca30dc101464fbf849df000af3ce7",
            "bfccbec20e5c43148f345c5f08e81d13",
            "2ac0a090c49c494fa9a7693afd9aab4a"
          ]
        },
        "id": "Mws6Z8NGcNJC",
        "outputId": "83dfd5be-69f2-4187-945a-b8893704ed47"
      },
      "source": [
        "# подсчитаем статистику по токенам, символам, для высказываний и диалогов\n",
        "# а также о частоте слов-токенов\n",
        "\n",
        "max_len_input = 0 # максимальная длина входа в encoder в символах\n",
        "max_length_utterance = 0 # максимальная длина высказывания в символах\n",
        "max_token_len_input = 0 # максимальная длина входа в encoder в токенах\n",
        "max_token_length_utterance = 0 # максимальная длина высказывания в токенах\n",
        "dialog_length_list = [] # список кортежей длин входа в encoder в символах и токенах\n",
        "utterance_length_list = [] # список кортежей длин высказываний в символах и токенах\n",
        "vocab_freq = Counter() # счетчик токенов\n",
        "total_utterance_in_dialogs = set() # множество (счетчик) уникальных высказываний\n",
        "ucode_in_dialogs = Counter() # счетчик кодов использованных в диалогах для проверки\n",
        "\n",
        "for dialog in tqdm(dialogs):\n",
        "    dialog_length = 0\n",
        "    dialog_token_length = 0\n",
        "\n",
        "    for utterance_code in dialog:\n",
        "        utterance = utterances[utterance_code]\n",
        "        total_utterance_in_dialogs.add(utterance)\n",
        "        ucode_in_dialogs[utterance_code] += 1\n",
        "        utterance_length = len(utterance)\n",
        "        tokens = prepare_string(utterance).split(' ')\n",
        "        for token in tokens:\n",
        "            vocab_freq[token] += 1\n",
        "        utterance_token_length = len(tokens)\n",
        "        dialog_length += utterance_length + 1 # +1 for an extra space between utterances in input_text\n",
        "        dialog_token_length += utterance_token_length\n",
        "        max_length_utterance = max(utterance_length, max_length_utterance)\n",
        "        max_token_length_utterance = max(utterance_token_length, \n",
        "                                         max_token_length_utterance)\n",
        "        utterance_length_list.append((utterance_length, utterance_token_length))\n",
        "\n",
        "    dialog_length -= utterance_length # remove the last utterance\n",
        "    dialog_token_length -= utterance_token_length\n",
        "    max_len_input = max(dialog_length, max_len_input)\n",
        "    max_token_len_input = max(dialog_token_length, max_token_len_input)\n",
        "    dialog_length_list.append((dialog_length, dialog_token_length))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 83097/83097 [00:04<00:00, 18870.90it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-kDF5WGixc5",
        "outputId": "47a15114-71cd-4747-a397-d0bc600a96e2"
      },
      "source": [
        "print(f'Max length of dialog except last utterance: {max_len_input} chars, {max_token_len_input} tokens')\n",
        "print(f'Max length of utterance: {max_length_utterance} chars, {max_token_length_utterance} tokens')\n",
        "print(f'Number of unique utterances: {len(total_utterance_in_dialogs)}')\n",
        "print(f'Total number utterances linked to dialog: {len(ucode_in_dialogs)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length of dialog except last utterance: 5899 chars, 1366 tokens\n",
            "Max length of utterance: 3046 chars, 730 tokens\n",
            "Number of unique utterances: 265774\n",
            "Total number utterances linked to dialog: 304713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67h_yA9S2ekI",
        "outputId": "f9bbd5d1-cabc-4c9b-8694-310acf7b6073"
      },
      "source": [
        "def get_tokens_number_from_frequency(freq_target):\n",
        "    return sum([1 for freq in vocab_freq.values() if freq == freq_target])\n",
        "\n",
        "for i in range(1,4):\n",
        "    print(f'Number of rare tokens (freq=={i}): {get_tokens_number_from_frequency(i)}')\n",
        "\n",
        "print(f'total number of unique tokens: {len(vocab_freq.keys())}')\n",
        "print(f'Total number of tokens: {sum(vocab_freq.values())}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rare tokens (freq==1): 43180\n",
            "Number of rare tokens (freq==2): 11682\n",
            "Number of rare tokens (freq==3): 5852\n",
            "total number of unique tokens: 85108\n",
            "Total number of tokens: 4034165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7jtOWmC8gIX",
        "outputId": "c168d5be-04c1-4cec-e011-3b7808855304"
      },
      "source": [
        "print(f'Statistics for number of utterances in dialog:')\n",
        "num_utter = [len(dialog) for dialog in dialogs]\n",
        "print(f'  min: {min(num_utter)}')\n",
        "print(f'  max: {max(num_utter)}')\n",
        "print(f'  avg: {np.mean(num_utter)}')\n",
        "print(f'  median: {np.median(num_utter)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics for number of utterances in dialog:\n",
            "  min: 2\n",
            "  max: 89\n",
            "  avg: 3.6669554857576085\n",
            "  median: 3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAAvEInXFpcd",
        "outputId": "28141a0f-ca37-4476-8cda-2539aed8adca"
      },
      "source": [
        "punc_token_counts = 0\n",
        "print('Frequency of punctuation tokens:')\n",
        "for i in '.,!?-':\n",
        "    print(f\"  '{i}': {vocab_freq.get(i)}\")\n",
        "    punc_token_counts += vocab_freq.get(i)\n",
        "print(f'Number of punctuation tokens: {punc_token_counts}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frequency of punctuation tokens:\n",
            "  '.': 503913\n",
            "  ',': 170375\n",
            "  '!': 37866\n",
            "  '?': 110240\n",
            "  '-': 6213\n",
            "Number of punctuation tokens: 828607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "dVm8WntikolB",
        "outputId": "657836cd-57cc-475c-fd59-bd3909862cd8"
      },
      "source": [
        "plt.figure(figsize=(25, 5))\n",
        "plt.subplot(1,3,1) # left plot\n",
        "x1, x2 = zip(*utterance_length_list)\n",
        "plt.hist([x1, x2], bins=30, label=['char', 'tokens'])\n",
        "plt.title('Utterance length in chars or tokens')\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.xticks(np.arange(0, 3200, step=200))\n",
        "plt.xlim(0,3200)\n",
        "\n",
        "plt.subplot(1,3,2) # right plot\n",
        "x1, x2 = zip(*dialog_length_list)\n",
        "plt.hist([x1, x2], bins=30, label=['char', 'tokens'])\n",
        "plt.title('Dialog length in chars or tokens (without last utterance)')\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.xticks(np.arange(0, 6000, step=400))\n",
        "plt.xlim(0,6000)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(vocab_freq.values(), bins=100,log=True)\n",
        "plt.title('Tokens frequency distribution')\n",
        "plt.xlabel('Tokens frequecy')\n",
        "plt.show()\n",
        "del x1; del x2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x360 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtOmrYB4ocXv"
      },
      "source": [
        "**Выводы EDA**:\n",
        "- количество диалогов - 83 097, половина из них содержит 2-3 высказывания, хотя есть и длинные диалоги (максимум 89 высказываний)\n",
        "- общее количество высказываний в файле - 304 713, из них 265 774 уникальных высказываний; также проверено, что все высказывания из 304 713 в файле используются в диалогах\n",
        "- общее количество токенов во всех диалогах - 4 034 134 (из них 828 602 токена - это знаки препинания)\n",
        "- всего 85 259 уникальных токенов (включая знаки препинания), из них половина (43,314 или 50.8%) встречаются лишь один раз.\n",
        "- максимальная длина высказывания 3 046 символов или 730 токенов\n",
        "- лишь незначительная часть высказываний длиннее 1 000 символов или 400 токенов\n",
        "\n",
        "Если в качестве тренировочной последовательности (input_text) скармливать от 1 до N-1 высказываний, а следующее за ним/ними - выдавать за целевую, то:\n",
        "- максимальная длина диалога на входе 5 899 символов или 1 366 токенов\n",
        "- большая часть диалогов укладывается в 700-800 токенов или 2 000-2 500 символов\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcBmohjQahhS"
      },
      "source": [
        "## Обзор вариантов модели чат-бота\n",
        "\n",
        "*Выводы сделаны после того, как были реализованы все 3 модели.*<br>\n",
        "*Во всех моделях LSTM слои Encoder'а двунаправленные, Decoder'а - однонаправленные.*\n",
        "\n",
        "1. посимвольный Encoder-Decoder без attention (как в примере, финальные параметры: 3 слоя LSTM, H_SIZE=1024, EMB_SIZE=128 => 64,644,702 training parameters).\n",
        "\n",
        "  Props:\n",
        "  - значительно(!) меньше параметров на Embedding слое(-ях) и выходном Dense слое decoder'а.\n",
        "\n",
        "  Cons:\n",
        "  - сложнее архитектура сети: так как символ не содержит смысла всего слова, в котором он содержится, и это надо компенсировать бОльшей глубиной сети (2-3 слоя минимум, возможно с residual connections для борьбы с затухающим градиентом) и большим количеством фичей RNN слоев (несколько сотен или тысяч)\n",
        "  - большая длина посимвольной последовательности: проблема градиентов и размер вычислительного графа - модель может просто не поместится в память GPU\n",
        "  \n",
        "  \n",
        "2. Encoder-Decoder без attention на токенах. (финальные параметры: 2 слоя LSTM, H_SIZE=2048, EMB_SIZE=256 => 246,100,172 training parameters)\n",
        "  \n",
        "  Props:\n",
        "  - на порядок короче длина цепочки\n",
        "  - меньше внутренних слоев моделей\n",
        "  - меньше затухания градиента из-за длины цепочек и меньшего количества слоев\n",
        "  - всё это освобождает место для большего количества скрытых параметров: H_SIZE\n",
        "  - возможность использовать специальные токены вроде `<NUM>` , `<PRICE>`, `<TIME>`, вместо которых можно подставлять сгенерированные значения (да и размер словарей это уменьшает) (это скорее вынужденная мера для уменьшения размера словарей и справедливости ради, то же самое можно было реализовать и для char-RNN)\n",
        "  \n",
        "  Cons:\n",
        "  - больше обучаемых параметров (переобучение?)\n",
        "  - в случае более сложных языков (с большим количеством форм слова) размер словарей увеличится многократно.\n",
        "  \n",
        "  \n",
        "3. Encoder-Decoder с attention на токенах. (финальные параметры: 1 LSTM слой, H_SIZE=2048, EMB_SIZE=256, ATT_UNITS=2048 => 125,936,472 training parameters)\n",
        "\n",
        "  Props (по сравнению с такой же моделью без attention):\n",
        "  - Распределение внимания по словам для лучшего извлечения смысла предложения - количественно сложно сравнить, так как изначальные диалоги жестко вырваны из контекста, и ответы бота всё равно часто не в попад (особенно длинные ответы, в то время как короткие - более подходят предыдущей реплике)\n",
        "  \n",
        "  Cons:\n",
        "  - значительное увеличение вычислительного графа при обучении и следовательно и ограничение на размер скрытых параметров, и/или длину цепочки (по сравнению со 2-ым вариантом максимальную длину цепочки пришлось уменьшить с 30 до 15 (одновременно уменьшая количество LSTM слоев с 2 до одного), чтоб вычислительный граф помещался в память GPU).\n",
        "  - фиксированная длина входной цепочки.\n",
        "\n",
        "Рассматриваемые варианты обучания:\n",
        "- с максимальной длиной диалогов (не хватает памяти), приходится ограничивать число итераций на эпоху, так как слишком много долгие эпохи\n",
        "- ограничивая длиной в 500 символов (не всегда хватает памяти в colab), продолжаем ограничивать число итераций на эпоху\n",
        "- ограничивая длиной в 100 символов, всё еще остается около 100 000 диалогов в выборке, но теперь можно попробовать не ограничивать итерации и выполнять эпохи на всех данных\n",
        "- обучение большими батчами (200-500) и минибатчами (менее 128)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gByfArbwH_A0"
      },
      "source": [
        "## Вариант 1. Char-RNN Encoder-decoder without attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFSUdRD3MIQE"
      },
      "source": [
        "### Подготовка данных: input-target\n",
        "\n",
        "Создадим списки из входных данных и соответствующих им целевых последовательностей.\n",
        "Есть два варианта определения вход-выход:\n",
        "1. Перебирая целевые высказывания от 2-го до последнего в диалоге, в качестве последовательности на входе берем предыдущее высказывание. Например, если далог состоит из 4 высказываний: `[a, b, c, d]` то пары input(I)-taget(T) будут:\n",
        "  - I: `a`, T: `b`\n",
        "  - I: `b`, T: `c`\n",
        "  - I: `c`, T: `d`\n",
        "2. Перебирая целевые высказывания от 2-го до последнего в диалоге, в качестве последовательности на входе берем все высказывания, идущие перед целевым.\n",
        "Например, если далог состоит из 4 высказываний: `[a, b, c, d]` то пары input(I)-taget(T) будут:\n",
        "  - I: `a`, T: `b`\n",
        "  - I: `a b`, T: `c`\n",
        "  - I: `a b c`, T: `d`\n",
        "\n",
        "Для большей глубины мысли в encoder'е выберем 2-й вариант. Но при этом не будем использовать двунаправленную RNN чтоб важность последнего высказывания имело больший вес (как оно и будет происходить в однонаправленной сети).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "f86489305644413586dc4a7b67320cc3",
            "d7cb4a49cb464119a8b3ec7a89597a1d",
            "f6ccc1b7e75546c99e470c065538be39",
            "debfa98416d840e28c8c89f686d31eda",
            "125439b8f63d4479b3b46185d69a541e",
            "b9aa21e4e62347d9bb5cecffa25dbe39",
            "036c68d1c56743b8b073176620274873",
            "9f30614f0c4c42a0a8a7661d1abdff43"
          ]
        },
        "id": "emOjWfsBH-cl",
        "outputId": "86b1613f-bab1-40c1-f059-6c4961044187"
      },
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "max_len_input, max_len_target = 0, 0\n",
        "MAX_SEQ_LEN = 100\n",
        "\n",
        "def prepare_text_for_charRNN(text):\n",
        "    return re.sub(r'[ \\t]+', \" \", text).strip()\n",
        "\n",
        "for dialog in tqdm(dialogs):\n",
        "    for i in range(1, len(dialog)):\n",
        "        # join (1 + ... + i-1) utterances in one input\n",
        "        input_text = ' '.join([utterances[dialog[j]] for j in range(i)])\n",
        "        # condition text\n",
        "        input_text = prepare_text_for_charRNN(input_text) \n",
        "        # assign i-th utterance as target and condition it\n",
        "        target_text = prepare_text_for_charRNN(utterances[dialog[i]])\n",
        "\n",
        "        # stop processing this dialog if input or target max length is reached\n",
        "        if max(len(input_text), len(target_text)) > MAX_SEQ_LEN:\n",
        "            break\n",
        "        \n",
        "        max_len_input = max(max_len_input, len(input_text))\n",
        "        max_len_target = max(max_len_target, len(target_text))\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "\n",
        "print('Length of input/target lists:', len(input_texts))\n",
        "print('Max length of input:', max_len_input)\n",
        "print('Max length of target:', max_len_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 83097/83097 [00:01<00:00, 70444.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of input/target lists: 99810\n",
            "Max length of input: 100\n",
            "Max length of target: 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoJ-Gs7djqvK"
      },
      "source": [
        "- 180550 elements with length <= 300\n",
        "- 154803 elements with length <= 200\n",
        "- 99810  elements with length <= **100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6u41OpmkcO_",
        "outputId": "80e6eccb-07f3-4979-92b0-6f850574e5af"
      },
      "source": [
        "i = np.random.randint(0, high=len(input_texts))\n",
        "print(f'Sample #{i}:')\n",
        "print('-', input_texts[i])\n",
        "print('-', target_texts[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample #72526:\n",
            "- -- Could you?\n",
            "- Never tried, 'til now. Joe, may I kiss you?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N85ldgOGd49r"
      },
      "source": [
        "Даже выбрав диалоги с последовательностями меньше 500 символов длиной, осталось 202 826 тренировочных примера, что полностью обойти полными эпохами не получится (2sec на батч из 128 последовательностей -> 53 минуты на одну полную эпоху)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVXrRV2PW4Lf"
      },
      "source": [
        "### Подготовка словаря\n",
        "\n",
        "Для первого варианта решения, в качестве элемента последовательности будем использовать один символ (а не слово).\n",
        "\n",
        "Подготовим два словаря (отображения индекса в символ и символа в индекс), и сделаем это для всех высказований из `input_texts` и `target_texts`.\n",
        "\n",
        "Кроме того, нам понадобятся специальные токены для начала и конца цепочки, и паддинга (`<START>`, `<END>`, `<PAD>`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "3beb66cec7404094a4d7bbd5fe9b52b7",
            "584cb4d956d44d639064013835f24d11",
            "2ce5fd057fe74f1f91da5f830d0029b5",
            "2853334608574eeeac831d9ed9cb84de",
            "4a2b2492457342a0b191611d6e71cd8e",
            "8a39e768a7404303a96ec52a1c562ce8",
            "582a330393644a35b58d1a64ce30847e",
            "6d3b75d10a9d46a2b5c18571b8307bc9"
          ]
        },
        "id": "KdTllVeCrQuI",
        "outputId": "becff45b-0708-410c-c097-55b0c6a061ce"
      },
      "source": [
        "vocab = set()\n",
        "for text in tqdm(input_texts + target_texts):\n",
        "    vocab.update(set(''.join(text)))\n",
        "vocab = sorted(vocab)\n",
        "vocab.append('<START>')\n",
        "vocab.append('<END>')\n",
        "vocab.append('<PAD>')\n",
        "VOCAB_SIZE = len(vocab)\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "print(f'Vocabulary size: {VOCAB_SIZE}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 199620/199620 [00:00<00:00, 449585.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 94\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dvz4PBoYKSy"
      },
      "source": [
        "### Подготовка обучающего датасета\n",
        "\n",
        "Наша модель будет состоять из двух частей: `Encoder` и `Decoder`. Задача энкодера считать входную цепочку и получить её закодированное представление. А задача декодера по этому закодированному представлению получить выходную цепочку.\n",
        "\n",
        "Декодер по сути является генератором текста, поэтому используется он аналогично тому, как мы это делали ранее с символьным генератором текста. Отличие только в том, что тут декодер будет получать начальное состояние из энкодера, а в качестве \"начала\" цепочки будет получать токен `<START>`.\n",
        "\n",
        "И точно так же, как и в случае с генератором, для обучения декодера в качестве входа и целевого выхода будем использовать одну и ту же цепочку, но сдвинутую на один элемент во времени. В конце Декодер должен предсказать токен `<END>`.\n",
        "\n",
        "Например, входом и выходом для декодера могут быть такие две цепочки из семи символов (начальный и конечный токен это один символ):\n",
        "\n",
        "`<START>Привет` --> `Привет<END>`\n",
        "\n",
        "Таким образом, для **обучения** `Encoder-Decoder` нам понадоятся три набора цепочек:\n",
        " - `encoder_input_seqs` - входы в Encoder\n",
        " - `decoder_input_seqs` - входы в Decoder\n",
        " - `decoder_target_seqs` - целевые выходы из Decoder (и всей модели Encoder-Decoder)\n",
        "\n",
        "Сами цепочки будут являться последовательностями целочисленных индексов (полученных с помощью соответствующих словарей).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptvZBL0DrjSA",
        "outputId": "f0559dea-2c81-47c5-cd0e-fb5f52e18fb1"
      },
      "source": [
        "%%time\n",
        "input_texts_as_int = [[char2idx[c] for c in text] for text in input_texts]\n",
        "target_texts_as_int = [[char2idx[c] for c in text] for text in target_texts]\n",
        "\n",
        "encoder_input_seqs = [np.array(text).astype(np.int8) for text in input_texts_as_int]\n",
        "decoder_input_seqs = []\n",
        "decoder_target_seqs = []\n",
        "for target_text in target_texts_as_int:\n",
        "    decoder_input_seqs.append(np.array([char2idx['<START>']] + target_text).astype(np.int8))\n",
        "    decoder_target_seqs.append(np.array(target_text + [char2idx['<END>']]).astype(np.int8))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.64 s, sys: 44 ms, total: 1.68 s\n",
            "Wall time: 1.69 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee7NkLh-a9Cc"
      },
      "source": [
        "### Паддинг цепочек\n",
        "\n",
        "Вспомним, что для обучения нам надо использовать батчи, которые состоят из цепочек одинаковой длины. А изначально длина цепочек (как входных, так и выходных) может быть произвольной. Поэтому нам необходимо сделать паддинг -- дополнить все цепочки до некоторой фиксированной длины с помощью символа `<PAD>`. В качестве длин будем брать фактическую максимальную длину последовательностей (отдельно для входных, отдельно для выходных). Напомню, что для тренировочных данных мы брали цепочки не блинее 500 символов. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMk-6fNdx5Ff"
      },
      "source": [
        "max_enc_seq_length = max([len(seq) for seq in encoder_input_seqs])\n",
        "max_dec_seq_length = max([len(seq) for seq in decoder_input_seqs])\n",
        "\n",
        "encoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    encoder_input_seqs,\n",
        "    value=char2idx['<PAD>'],\n",
        "    padding='post',\n",
        "    maxlen=max_enc_seq_length).astype(np.int8)\n",
        "\n",
        "decoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    decoder_input_seqs,\n",
        "    value=char2idx['<PAD>'],\n",
        "    padding='post',\n",
        "    maxlen=max_dec_seq_length).astype(np.int8)\n",
        "\n",
        "decoder_target_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    decoder_target_seqs,\n",
        "    value=char2idx['<PAD>'],\n",
        "    padding='post',\n",
        "    maxlen=max_dec_seq_length).astype(np.int8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYAnOuhFdnlT",
        "outputId": "2772305a-ae5e-4dcd-f47e-40c68c670f79"
      },
      "source": [
        "encoder_input_seqs.shape, decoder_input_seqs.shape, decoder_target_seqs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((99810, 100), (99810, 101), (99810, 101))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY-yw6zNcWdK"
      },
      "source": [
        "### Создание модели\n",
        "\n",
        "Для создания Encoder-Decoder модели воспользуемся смесью двух стилей: реализация моделей через собственный класс и функциональный API. \n",
        "\n",
        "Сами по себе Encoder и Decoder (по отдельности) удобно реализовать в виде кастомных классов (наследованных от `tf.keras.Model`), так как у них может быть какая-то сложная реализация. При этом у них будет общий Embedding.\n",
        "\n",
        "В нашем случае Encoder будет состоять из двух LSTM слоей, который будет возвращать финальное состояние после прохода по всей цепочке. В качестве состояния нас интересует и вектор `h` и вектор состояния LSTM `c`. Для него нам понадобится дополнительный флаг `return_state=True`\n",
        "\n",
        "В Декодере будет два LSTM и полносвязный слой для генерации финальных ответов (распределение вероятностей по символам). Для прямого распространения (`__call__`) кроме входной цепочки декодер будет получать состояние от энкодера (`init_state`) и будет передавать его в свой LSTM слоя в качестве начального состояния, а возвращать будет предсказанную выходную цепочку (той же длины, return_sequences=True) состояние этого LSTM.\n",
        "\n",
        "После того, как мы отдельно построили Encoder и Decoder, надо соединить их в Encoder-Decoder. Но так как нам нужно создать несколько входов в модель (отдельно входная цепочка в энкодер, отдельно входная цепочка в декодер) очень удобно сделать это с помощью функционального API. Фходные узлы создаются с помощью `tf.keras.layers.Input`, а затем строим вычислительный граф, используя модели `encoder_model` и `decoder_model`.\n",
        "\n",
        "Используем разные `Embedding` слои для encoder и decoder, несмотря на то, что язык общения в диалогах один и тотже, для encoder'а и decoder'а они по факту разные. Также на входе и encoder и decoder ставим `Masking` слой для \"отрезания\" паддингов (чтобы обратное распространение градиентов по паддингам не считалось).\n",
        "\n",
        "Финальная модель -- `seq2seq`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxaSL4NGfdOC"
      },
      "source": [
        "# попробовать раздельные Embedding слои большего разрешения\n",
        "\n",
        "H_SIZE = 1024 # Размерность скрытого состояния LSTM\n",
        "EMB_SIZE = 128 # размерность эмбеддингов (и для входных и для выходных цепочек)\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, is_bidirectional=False, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.masking = tf.keras.layers.Masking(mask_value=char2idx['<PAD>'])\n",
        "        self.embed = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.lstm2 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.lstm3 = tf.keras.layers.LSTM(H_SIZE, return_sequences=False, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.add_layer = tf.keras.layers.Add()\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "\n",
        "        # bidirectional mode is sum to return that same shape of H, C as for single dir.\n",
        "        if self.is_bidirectional:\n",
        "            self.lstm1 = tf.keras.layers.Bidirectional(self.lstm1, merge_mode='sum')\n",
        "            self.lstm2 = tf.keras.layers.Bidirectional(self.lstm2, merge_mode='sum')\n",
        "            self.lstm3 = tf.keras.layers.Bidirectional(self.lstm3, merge_mode='sum')       \n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.masking(x)\n",
        "        out = self.embed(out)\n",
        "\n",
        "        if self.is_bidirectional:\n",
        "            out1, h1_f, c1_f, h1_b, c1_b = self.lstm1(out)\n",
        "            h1 = h1_f + h1_b\n",
        "            c1 = c1_f + c1_b\n",
        "            \n",
        "            out2, h2_f, c2_f, h2_b, c2_b = self.lstm2(out1)\n",
        "            h2 = h2_f + h2_b\n",
        "            c2 = c2_f + c2_b\n",
        "            \n",
        "            out_sum = self.add_layer([out1, out2])\n",
        "            \n",
        "            out3, h3_f, c3_f, h3_b, c3_b = self.lstm3(out_sum)\n",
        "            h3 = h3_f + h3_b\n",
        "            c3 = c3_f + c3_b\n",
        "            \n",
        "        else:\n",
        "            out1, h1, c1 = self.lstm1(out)\n",
        "            out2, h2, c2 = self.lstm2(out1)\n",
        "            out_sum = self.add_layer([out1, out2])\n",
        "            _, h3, c3 = self.lstm3(out_sum)\n",
        "            \n",
        "        state = ((h1, c1), (h2, c2), (h3, c3))\n",
        "        return state\n",
        "\n",
        "    \n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.masking = tf.keras.layers.Masking(mask_value=char2idx['<PAD>'])\n",
        "        self.embed = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.lstm2 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.lstm3 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.fc = tf.keras.layers.Dense(VOCAB_SIZE, activation='softmax')\n",
        "        self.add_layer = tf.keras.layers.Add()\n",
        "        \n",
        "    def call(self, x, init_state):\n",
        "        out = self.masking(x)\n",
        "        out = self.embed(out)\n",
        "        out1, h1, c1 = self.lstm1(out, initial_state=init_state[0])\n",
        "        out2, h2, c2 = self.lstm2(out1, initial_state=init_state[1])\n",
        "        out_sum = self.add_layer([out1, out2]) # residual connection\n",
        "        out, h3, c3 = self.lstm3(out_sum, initial_state=init_state[2])\n",
        "        out = self.fc(out)\n",
        "        state = ((h1, c1), (h2, c2), (h3, c3))\n",
        "        return out, state\n",
        "\n",
        "DROPOUT_RATE = 0.0\n",
        "encoder_model = Encoder(is_bidirectional=True, dropout=DROPOUT_RATE)\n",
        "decoder_model = Decoder(dropout=DROPOUT_RATE)\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "enc_state = encoder_model(encoder_inputs)\n",
        "decoder_outputs, _ = decoder_model(decoder_inputs, enc_state)\n",
        "\n",
        "seq2seq = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLS_nIPymKN7",
        "scrolled": true,
        "outputId": "083e9ed1-5b6d-4642-c1d9-af68b6f19442"
      },
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0005, clipnorm=5.0) # clip gradients to avoid g explosion\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "seq2seq.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    root_folder + 'models/model-charRNN/checkpoint.hdf5',\n",
        "    monitor='loss', save_weights_only=True)\n",
        "\n",
        "seq2seq.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Encoder)               (((None, 1024), (Non 43028224    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Decoder)               ((None, None, 94), ( 21616478    input_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 64,644,702\n",
            "Trainable params: 64,644,702\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27DDBuRUoC5-"
      },
      "source": [
        "### Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KNFbNq6eR8h",
        "outputId": "4fb35d92-2281-4fb6-f91d-f9725d3b9e13"
      },
      "source": [
        "# change to True to continue training from checkpoint\n",
        "# or save weights if checkpoint fails to do it\n",
        "restore_saved_model = True\n",
        "\n",
        "if restore_saved_model:\n",
        "    seq2seq.load_weights(\n",
        "        root_folder + 'models/model-charRNN/checkpoint.hdf5'\n",
        "        )\n",
        "    print('Weights restored.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights restored.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RB-0JWC39HE"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1\n",
        "\n",
        "# enter last epoch number completely finished\n",
        "epochs_counter = 140\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(((encoder_input_seqs, decoder_input_seqs), decoder_target_seqs))\n",
        "train_ds = train_ds.shuffle(encoder_input_seqs.shape[0]).repeat().batch(BATCH_SIZE)\n",
        "# placing batch after repeat we concatenate last (potentially incomplete) batch \n",
        "# with data from the next repeat (or epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rT1D_MQYxKm",
        "scrolled": true,
        "outputId": "ec4eb6a4-8ebe-4ec3-c133-59dcf062fc77"
      },
      "source": [
        "# данных настолько много, что в эпохе 3000+ итераций (больше часа на 1 эпоху)\n",
        "steps_per_epoch = encoder_input_seqs.shape[0] // BATCH_SIZE\n",
        "\n",
        "# seq2seq.fit([encoder_input_seqs, decoder_input_seqs], decoder_target_seqs,\n",
        "#           batch_size=BATCH_SIZE,\n",
        "#           epochs=EPOCHS+epochs_counter, initial_epoch=epochs_counter,\n",
        "#           steps_per_epoch=steps_per_epoch, callbacks=[checkpoint])\n",
        "\n",
        "\n",
        "seq2seq.fit(train_ds, epochs=EPOCHS + epochs_counter, \n",
        "            initial_epoch=epochs_counter, steps_per_epoch=steps_per_epoch, \n",
        "            callbacks=[checkpoint]) \n",
        "\n",
        "epochs_counter += EPOCHS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 141/141\n",
            "1559/1559 [==============================] - 1011s 648ms/step - loss: 0.2103 - accuracy: 0.9317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkGCb4ATBFdg"
      },
      "source": [
        "# run it if the last epoch showed greater loss than previous checkpoint\n",
        "backup_model = True\n",
        "\n",
        "if backup_model:\n",
        "    seq2seq.save_weights(\n",
        "        root_folder + 'models/model-charRNN/checkpoint.hdf5'\n",
        "       )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5M_8rngc137"
      },
      "source": [
        "### Функция для инференса\n",
        "\n",
        "Запуск инференса для Encoder-Decoder состоит из последовательного применения энкодера и декодера. \n",
        "\n",
        "Сначала прогоняем входную цепочку через энкодер и получаем закодированное представление `state`.\n",
        "\n",
        "А дальше применяем декодер в похожем режиме, как это было с генератором текста (только теперь передаём `state` в качестве начального состояния). В цикле постепенно генерируем выходную цепочку, подавая в декодер лишь один (текущий) символ и получая один предсказанный (следующий) символ. Начинаем с символа `<START>` и повторяем до тех пор, пока не получим символ `<END>` на выходе или не достигнем лимита по количеству символов в цепочке. Для определения того, какой символ предсказал декодер, просто воспользуемся  функцией `argmax` для выходного распределения (выхода FC слоя)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-jCrL-_ad2V"
      },
      "source": [
        "def mix_states(state1, state2, weights=None):\n",
        "    \"\"\"\n",
        "    Both states is expected to be a tuple of N tuples of pairs, where N is a \n",
        "    number of hidden RNN layers in encoder/decoder, and pairs are tuples of \n",
        "    hidden state (h, c) (for LSTM layer).\n",
        "    If hidden state doesn't have memory state (like GRU), then \n",
        "    state is expected to be like this: (h, )\n",
        "    default weights = [0.5, 0.5]. They're weights to mix state1 and state2 in\n",
        "    its weigthed average.\n",
        "    For example, two LSTM layers model should have state = ((h1, c1), (h2, c2))\n",
        "    where h1,c1 - state of the 1st LSTM layer, h2,c2 - state of 2nd LSTM layer.\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        weights = [0.5, 0.5]\n",
        "    dim1 = len(state1) # number of hidden layers\n",
        "    out = []\n",
        "    for i in range(dim1):\n",
        "        layer_state = []\n",
        "        for j in range(len(state1[i])):\n",
        "            layer_state.append(\n",
        "                weights[0] * state1[i][j] + weights[1] * state2[i][j]\n",
        "            )\n",
        "        out.append(tuple(layer_state))\n",
        "    return tuple(out)\n",
        "\n",
        "\n",
        "def seq2seq_inference(input_seq, state_init=None, weights=None):\n",
        "    max_length = 100\n",
        "    input_seq_cor = re.sub(r'[\\n]', '', input_seq) # delete \\n as it's not in vocab\n",
        "    input_seq_cor = prepare_text_for_charRNN(input_seq_cor)\n",
        "    input_seq_cor = np.array([[char2idx[i] for i in input_seq_cor]]).astype(np.int8)\n",
        "    state = encoder_model(input_seq_cor)\n",
        "    \n",
        "    \n",
        "    \n",
        "    if state_init is not None:\n",
        "        if weights is None:\n",
        "            weights = [1, 0.3]\n",
        "        state = mix_states(state, state_init, weights=weights)\n",
        "\n",
        "    target_seq = np.array([[char2idx['<START>']]])\n",
        "\n",
        "    decoded_sentence = ''\n",
        "    while True:\n",
        "        output_tokens, state = decoder_model(target_seq, state)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = idx2char[sampled_token_index]\n",
        "\n",
        "        if (sampled_char == '<END>' or\n",
        "           len(decoded_sentence) > max_length):\n",
        "            break\n",
        "        if sampled_char == '<PAD>':\n",
        "            sampled_char = ' '\n",
        "            \n",
        "        decoded_sentence += sampled_char\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "    \n",
        "    # if output is null run it again with slightly different (mixed) state\n",
        "    if decoded_sentence == '':\n",
        "        decoded_sentence, state = seq2seq_inference(input_seq[:np.random.randint(len(input_seq))], state)\n",
        "        \n",
        "    return decoded_sentence, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t9s1xtyaLBD"
      },
      "source": [
        "*U (user), B(bot)*\n",
        "\n",
        "**Initial model (3 layers, no connections)** (checkpoint.hdf5)\n",
        "\n",
        "After 28 epochs (14000 iterations total)\n",
        "\n",
        "  U: What is your name?<br>\n",
        "  B: I don't know. I was thinking of that thing I want to do with you.\n",
        "\n",
        "After 60 epochs (30000 iters):\n",
        "\n",
        "U: What is your name?<br>\n",
        "B: I don't know. I don't know what the hell is going on.\n",
        "\n",
        "After 70 epochs:\n",
        "\n",
        "U: What is your name?<br>\n",
        "B: I don't know. I was just talking about the same thing.\n",
        "\n",
        "Next 10 epochs showed that model fell into local minimum (RSMProp, LR=0.001, max_seq_len changed from 500 to 300 due to memory constrains)... On all checkpoints model tends to answer closely the same thing regardless of input.\n",
        "\n",
        "**Adding resudual connection from lstm1 output to lstm3 input. Change optimizer from RSMProp to Adam. Training from scratch on 300-len sequences** (checkpoint2.hdf5)\n",
        "\n",
        "after 10 epochs (200 batch_size):\n",
        "\n",
        "U: are you stupid?<br>\n",
        "B: I don't know. I want to see you and I want to see you.\n",
        "\n",
        "After 28 epochs (128 batch_size):\n",
        "\n",
        "U: are you stupid?<br>\n",
        "B: I don't know. *(inf)\n",
        "\n",
        "**Use bidirectional LSTM layers in encoder (mode='sum'). Reduce EMB_SIZE from 256 to 128, increase H_SIZE from 512 to 1024, reduce sequence_max_length from 300 to 100. Training on full dataset without limitting steps_per_epoch**\n",
        "\n",
        "after 11 epochs (batch_size=128, 8569 training steps)\n",
        "\n",
        "U: are you stupid?<br>\n",
        "B: I don't know. I just want to see him.\n",
        "\n",
        "after 21 epochs (batch_size=64, 24159 training steps)\n",
        "\n",
        "U: are you stupid?<br>\n",
        "B: [thua!!!\n",
        "\n",
        "after 33 epochs (batch_size=64, 42867 training steps)\n",
        "\n",
        "U: are you stupid?<br>\n",
        "B: What do you mean?\n",
        "\n",
        "after 43 epochs (batch_size=64, 58457 training steps)\n",
        "\n",
        "U: are you stupid?<br>\n",
        "B: -- You can't be --\n",
        "\n",
        "after 73 epochs (105227 training steps, weighting states 50/50):\n",
        "\n",
        "U: ya ne hochu s toboy obshat'sya.<br>\n",
        "B: You want to go talk to me?<br>\n",
        "U: Sure, I do.<br>\n",
        "B: ! You can't be here on the phone right now!!?<br>\n",
        "U: I'm in your head. Ha-ha-ha.<br>\n",
        "B: UNm, BUT YOUM MONYSAGS IT'S BAVY TH MATH THIS CAUSE.<br>\n",
        "U: yes, you are stupid. lol<br>\n",
        "B: UNU!!!<br>\n",
        "U: How old are you?<br>\n",
        "B: He's not a copy of this stuff, Doc.\n",
        "\n",
        "after 83 epochs (120817 training steps, weighting states 60/40 + random trim of input if output is null):\n",
        "\n",
        "U: Hello, Machine.<br>\n",
        "B: He was a lady. I got a little stranger than you.<br>\n",
        "U: Who are you talking about. Answer me!<br>\n",
        "B: nna!<br>\n",
        "U: You are not talkative, aren't you?<br>\n",
        "B: iguy, if you don't mind...sich.<br>\n",
        "U: Silence is gold. you're cool.<br>\n",
        "B: e--you got that right this morning, and the damn thing Systems call it.<br>\n",
        "U: Is he cool?<br>\n",
        "B: I don't know. I guess I just got to go. I'm sure Josh.<br>\n",
        "U: Bye. See you around!<br>\n",
        "B: I don't know. I guess I'll see you again.\n",
        "\n",
        "after 130 epochs:\n",
        "\n",
        "User: I will let you talk, guys!<br>\n",
        "Bot1: I don't know. I don't know. I don't know. I just want to see all my ship. I am giving you later.<br>\n",
        "Bot2: It was an accident.<br>\n",
        "Bot1: It was a brakey one. I wanted to see what you must have gone.<br>\n",
        "Bot2: I'll take that as a bowling.<br>\n",
        "Bot1: Understood<br>\n",
        "Bot2: please. Stay away from me.<br>\n",
        "Bot1: No! It was a bad idea! How come you're not at people?<br>\n",
        "Bot2: I'll take that as a contest pot.<br>\n",
        "Bot1: For the very momories are everything.<br>\n",
        "Bot2: I'll talk to you later.<br>\n",
        "Bot1: Understand what happened to me.<br>\n",
        "Bot2: Under the focushet office.\n",
        "\n",
        "User: please. Stay away from me.<br>\n",
        "Bot1: No. I want to see the sun.<br>\n",
        "Bot2: What do you mean?<br>\n",
        "Bot1: phburg coconut man. New Belled with the Rest.<br>\n",
        "Bot2: No. I want to see the sun.<br>\n",
        "Bot1: Ucked me.<br>\n",
        "Bot2: pity my fingers and a rich man.<br>\n",
        "Bot1: Un-friend. Come on, girls --<br>\n",
        "Bot2: I'm sorry, I didn't mean to.<br>\n",
        "Bot1: Uss me. I missed you to marry me.<br>\n",
        "Bot2: I'll be right back.<br>\n",
        "Bot1: Uc! Back to the money!<br>\n",
        "Bot2: What are you doing here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtUejpB6c6Hx"
      },
      "source": [
        "### Пример инференса\n",
        "\n",
        "Попробуем инференс Seq2Seq модели на цепочках из нашего датасета."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGydWFZroC6A"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gdTwSl_gaf9o",
        "outputId": "4387faa6-27df-4d1e-9246-9f332974746d"
      },
      "source": [
        "state = None\n",
        "decoded_sentence = 'Hello. How are you?' # input('User: ')\n",
        "print(f'User: {decoded_sentence}')\n",
        "for i in range(2):\n",
        "#     decoded_sentence = input('User: ')\n",
        "    decoded_sentence, state = seq2seq_inference(decoded_sentence, state, weights=[0.7, 0.3])\n",
        "    time.sleep(2)\n",
        "    print('Bot1:', decoded_sentence)\n",
        "    decoded_sentence, state = seq2seq_inference(decoded_sentence, state, weights=[0.9, 0.1])\n",
        "    time.sleep(2)\n",
        "    print('Bot2:', decoded_sentence)\n",
        "    \n",
        "    # reset state every 3rd cycle\n",
        "    if (i % 3 == 0) and (i > 0):\n",
        "        state = None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User: Hello. How are you?\n",
            "Bot1: I don't know. I think I've found a habit story.\n",
            "Bot2: Yeah, well, I've been around the bend a bit.\n",
            "Bot1: Ymy'd? This payment --\n",
            "Bot2: You don't have to tell me about the <u>bast</u> for the wedding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5KonLtaoC6A"
      },
      "source": [
        "## Вариант 2. Token-RNN Encoder-decoder without attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpavLy-FoC6A"
      },
      "source": [
        "### Подготовка данных: input-target\n",
        "\n",
        "Создадим списки из входных данных и соответствующих им целевых последовательностей.\n",
        "Есть два варианта определения вход-выход:\n",
        "1. Перебирая целевые высказывания от 2-го до последнего в диалоге, в качестве последовательности на входе берем предыдущее высказывание. Например, если далог состоит из 4 высказываний: `[a, b, c, d]` то пары input(I)-taget(T) будут:\n",
        "  - I: `a`, T: `b`\n",
        "  - I: `b`, T: `c`\n",
        "  - I: `c`, T: `d`\n",
        "2. Перебирая целевые высказывания от 2-го до последнего в диалоге, в качестве последовательности на входе берем все высказывания, идущие перед целевым.\n",
        "Например, если далог состоит из 4 высказываний: `[a, b, c, d]` то пары input(I)-taget(T) будут:\n",
        "  - I: `a`, T: `b`\n",
        "  - I: `a b`, T: `c`\n",
        "  - I: `a b c`, T: `d`\n",
        "\n",
        "Для большей глубины мысли в encoder'е выберем 2-й вариант. Но при этом не будем использовать двунаправленную RNN чтоб важность последнего высказывания имело больший вес (как оно и будет происходить в однонаправленной сети).\n",
        "\n",
        "Каждое высказывание подготавливаем для токенизации: числа заменяем на `<NUM>`, время на `<TIME>`, стоимость на `<PRICE>`, удаляем не пунктуационные знаки `&gt; &lt; &quot; * ~ | { } _ [ ]`, удаляем лишние пробелы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB8Xc04noC6B",
        "outputId": "5aa351d6-962e-407c-f3e6-0563c2d3a51d"
      },
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "max_len_input, max_len_target = 0, 0\n",
        "MAX_SEQ_LEN = 40\n",
        "\n",
        "def prepare_string(s):\n",
        "    \"\"\"\n",
        "    Prepair utterance for tokenization.\n",
        "    \"\"\"\n",
        "    # separate punctuation\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"([?.!,-])\", r\" \\1 \", s)\n",
        "    # remove special characters\n",
        "    s = re.sub(r\"(&gt;)|(&lt;)|(&quot;)|([\\*~|{}_\\[\\]`])\", '', s)\n",
        "    # substitute '&' to 'and'\n",
        "    s = re.sub(r'\\s&\\s', ' and ', s)\n",
        "    s = re.sub(r\"(?<=\\s)'(?=[a-z])|^'(?=[a-z])\", \"\", s) # remove ' at the beginning of the word\n",
        "    s = re.sub(r\"(<[A-Za-z]+>)|(</[A-Za-z]+>)\", \"\", s) # remove html-style tags\n",
        "    s = re.sub(r\"(^\\$[0-9]+)|([0-9]+\\$)\", ' <PRICE> ', s) # replace prices with <PRICE> like $5 or 10$\n",
        "    s = re.sub(r'([0-9]{1,2}h)|([0-9]{1,2}:[0-9]{2})', ' <TIME> ', s) # time formats like 5h, 10:35 to <TIME>\n",
        "    s = re.sub(r\"(?<=[0-9])(['%])\", r\" \\g<1>\", s) # separate % and ' from numbers, like \"007's\" or \"10%\"\n",
        "    s = re.sub(r\"[0-9]+-?((th)|(nd)|(st)|(rd))\", \" <NUM> \", s) # 1st, 2nd, 3rd, 10th or 10-th replace with <NUM>\n",
        "    s = re.sub(r\"(?<=[0-9])([A-Za-z]+)\", r' \\g<1>', s) # separate units like 33mm 300W\n",
        "    s = re.sub(r\"(#[0-9]+)|([0-9]+)\", ' <NUM> ', s) # replace numbers with <NUM>\n",
        "    # repalace double spaces and \\t with single space\n",
        "    s = re.sub(r'[\" \"\\t]+', \" \", s).strip()\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "for dialog in tqdm(dialogs):\n",
        "    for i in range(1, len(dialog)):\n",
        "        # join (1 + ... + i-1) utterances in one input\n",
        "        input_text = ' '.join([utterances[dialog[j]] for j in range(i)])\n",
        "        # condition text\n",
        "        input_text = prepare_string(input_text).split(' ')\n",
        "        # assign i-th utterance as target and condition it\n",
        "        target_text = prepare_string(utterances[dialog[i]]).split(' ')\n",
        "\n",
        "        # stop processing this dialog if input or target max length is reached\n",
        "        if max(len(input_text), len(target_text)) > MAX_SEQ_LEN:\n",
        "            break\n",
        "        \n",
        "        max_len_input = max(max_len_input, len(input_text))\n",
        "        max_len_target = max(max_len_target, len(target_text))\n",
        "\n",
        "        # it was noticed that some utterances are empty, for example L474\n",
        "        if (len(target_text) == 1) and (target_text[0] == ''):\n",
        "            # skip empty lines\n",
        "            continue\n",
        "        if (len(input_text) == 1) and (input_text[0] == ''):\n",
        "            # skip empty lines\n",
        "            continue\n",
        "\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "\n",
        "print('Length of input/target lists:', len(input_texts))\n",
        "print('Max length of input:', max_len_input)\n",
        "print('Max length of target:', max_len_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 83097/83097 [00:13<00:00, 5946.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of input/target lists: 141700\n",
            "Max length of input: 40\n",
            "Max length of target: 40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdam9je0oC6B"
      },
      "source": [
        "Проведем небольшой анализ объема тренировочных данных с разной максимальной длиной последовательности токенов:\n",
        "- MAX_SEQ_LEN=100 -> 196 239 samples\n",
        "- MAX_SEQ_LEN=50 -> 158 139 samples\n",
        "- MAX_SEQ_LEN=40 -> 141 700 samples\n",
        "- MAX_SEQ_LEN=30 -> 118 473 samples\n",
        "- MAX_SEQ_LEN=20 -> 84 186 samples\n",
        "\n",
        "Чтобы избежать слишком большого объема тренировочных данных, ограничим максимальное количество токенов с высказываниях 40-ю. При этом количество диалогов чуть превышает объем тренировочных данных, используемых в Char-RNN, но это оправдано гораздо большим количеством различных токенов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eaSGQTIoC6B",
        "outputId": "235dcb26-8345-475e-efd6-9d29cf586ced"
      },
      "source": [
        "# random sample from training set\n",
        "i = np.random.randint(0, high=len(input_texts))\n",
        "print(f'Sample #{i}:')\n",
        "print('-', ' '.join(input_texts[i]))\n",
        "print('-', ' '.join(target_texts[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample #16688:\n",
            "- the ah . . . the radio's out .\n",
            "- what ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZggpxmY0oC6B"
      },
      "source": [
        "### Подготовка словаря\n",
        "\n",
        "Одно слово или знак препинания - один токен.\n",
        "\n",
        "Для входа (input) и выхода (target) подготовим два словаря (отображения индекса в токен и токен в индекс), и сделаем это для всех высказований из `input_texts` и `target_texts`.\n",
        "\n",
        "Кроме того, нам понадобятся специальные токены для начала и конца цепочки, паддинга и незнакомых слов: `<START>`, `<END>`, `<PAD>`, `<UNK>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43O97XpJoC6B",
        "outputId": "7d6847c8-7f09-4098-9b86-c60440587dd1"
      },
      "source": [
        "def prepare_vocab(texts, is_input=False):\n",
        "    vocab = set()\n",
        "    counter = Counter()\n",
        "    for text in texts:\n",
        "        # for input_texts with replace irrelevant tokens with <UNK>\n",
        "        # token should start with letter or basic punctuation ?.,!-\n",
        "        # if is_input:\n",
        "        #     text = [s if re.match(r'^[A-Za-z?.,!-]', s) else '<UNK>'  for s in text]\n",
        "        vocab.update(set(text))\n",
        "        for token in text:\n",
        "            counter[token] += 1\n",
        "    # для target датасета добавим теги начала и конца последовательности\n",
        "    if not is_input:\n",
        "        vocab.add('<START>')\n",
        "        vocab.add('<END>')\n",
        "    \n",
        "    vocab.add('<PAD>')\n",
        "    vocab.add('<UNK>')\n",
        "    \n",
        "    # обязательно сортируем токены, так как set() не является упорядоченным типом данных, \n",
        "    # т.е. при разных запусках получим разные индексы\n",
        "    vocab = sorted(vocab)\n",
        "    \n",
        "    vocab_size = len(vocab)\n",
        "    token2idx = {u:i for i, u in enumerate(vocab)}\n",
        "    idx2token = np.array(vocab)\n",
        "    \n",
        "    return vocab_size, token2idx, idx2token, counter\n",
        "\n",
        "\n",
        "INPUT_VOCAB_SIZE, input_token2idx, input_idx2token, input_counter = prepare_vocab(input_texts, is_input=True)\n",
        "TARGET_VOCAB_SIZE, target_token2idx, target_idx2token, target_counter = prepare_vocab(target_texts)\n",
        "\n",
        "print(f'Input vocab size: {INPUT_VOCAB_SIZE}')\n",
        "for i in range(2):\n",
        "    rare_tokens = sum([1 for freq in input_counter.values() if freq == i+1])\n",
        "    print(f'  Tokens with frequency {i+1}:  {rare_tokens} ({rare_tokens/INPUT_VOCAB_SIZE*100:.2f}%)')\n",
        "print(f'Target vocab size: {TARGET_VOCAB_SIZE}')\n",
        "for i in range(2):\n",
        "    rare_tokens = sum([1 for freq in target_counter.values() if freq == i+1])\n",
        "    print(f'  Tokens with frequency {i+1}:  {rare_tokens} ({rare_tokens/TARGET_VOCAB_SIZE*100:.2f}%)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input vocab size: 33725\n",
            "  Tokens with frequency 1:  9339 (27.69%)\n",
            "  Tokens with frequency 2:  5864 (17.39%)\n",
            "Target vocab size: 34764\n",
            "  Tokens with frequency 1:  15624 (44.94%)\n",
            "  Tokens with frequency 2:  5275 (15.17%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ebnalnAoC6C"
      },
      "source": [
        "del input_counter\n",
        "del target_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQneNW9koC6C"
      },
      "source": [
        "В полном наборе данных было 85к уникальных токенов. В данной выборке количество одноразовых токенов хоть и снизилось (в том числе из-за подготовки текста), но их процент остался на прежнем уровне (около 50% на target_texts)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWyg5oCtoC6C"
      },
      "source": [
        "### Подготовка обучающего датасета\n",
        "\n",
        "Наша модель будет состоять из двух частей: `Encoder` и `Decoder`. Задача энкодера считать входную цепочку и получить её закодированное представление. А задача декодера по этому закодированному представлению получить выходную цепочку.\n",
        "\n",
        "Декодер по сути является генератором текста, поэтому используется он аналогично тому, как мы это делали ранее с символьным генератором текста. Отличие только в том, что тут декодер будет получать начальное состояние из энкодера, а в качестве \"начала\" цепочки будет получать токен `<START>`.\n",
        "\n",
        "И точно так же, как и в случае с генератором, для обучения декодера в качестве входа и целевого выхода будем использовать одну и ту же цепочку, но сдвинутую на один элемент во времени. В конце Декодер должен предсказать токен `<END>`.\n",
        "\n",
        "Например, входом и выходом для декодера могут быть такие две цепочки из семи символов (начальный и конечный токен это один символ):\n",
        "\n",
        "`<START>Привет` --> `Привет<END>`\n",
        "\n",
        "Таким образом, для **обучения** `Encoder-Decoder` нам понадобятся три набора цепочек:\n",
        " - `encoder_input_seqs` - входы в Encoder\n",
        " - `decoder_input_seqs` - входы в Decoder\n",
        " - `decoder_target_seqs` - целевые выходы из Decoder (и всей модели Encoder-Decoder)\n",
        "\n",
        "Сами цепочки будут являться последовательностями целочисленных индексов (полученных с помощью соответствующих словарей)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUpt-55LoC6C",
        "outputId": "a23b0084-f4f3-446a-9461-ce869249b067"
      },
      "source": [
        "%%time\n",
        "input_texts_as_int = [[input_token2idx.get(token, input_token2idx['<UNK>']) for token in text] for text in input_texts]\n",
        "target_texts_as_int = [[target_token2idx.get(token, target_token2idx['<UNK>']) for token in text] for text in target_texts]\n",
        "\n",
        "encoder_input_seqs = [np.array(text) for text in input_texts_as_int]\n",
        "decoder_input_seqs = []\n",
        "decoder_target_seqs = []\n",
        "for target_text in target_texts_as_int:\n",
        "    decoder_input_seqs.append(np.array([target_token2idx['<START>']] + target_text))\n",
        "    decoder_target_seqs.append(np.array(target_text + [target_token2idx['<END>']]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.65 s, sys: 44.1 ms, total: 1.69 s\n",
            "Wall time: 1.69 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBLeECPdoC6C"
      },
      "source": [
        "### Паддинг цепочек\n",
        "\n",
        "Вспомним, что для обучения нам надо использовать батчи, которые состоят из цепочек одинаковой длины. А изначально длина цепочек (как входных, так и выходных) может быть произвольной. Поэтому нам необходимо сделать паддинг -- дополнить все цепочки до некоторой фиксированной длины с помощью токена `<PAD>`. В качестве длин будем брать фактическую максимальную длину последовательностей (отдельно для входных, отдельно для выходных). Напомню, что для тренировочных данных мы брали цепочки не длинее 40 токенов. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zllj7meMoC6C"
      },
      "source": [
        "max_enc_seq_length = max([len(seq) for seq in encoder_input_seqs])\n",
        "max_dec_seq_length = max([len(seq) for seq in decoder_input_seqs])\n",
        "\n",
        "encoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    encoder_input_seqs,\n",
        "    value=input_token2idx['<PAD>'],\n",
        "    padding='post',\n",
        "    maxlen=max_enc_seq_length)\n",
        "\n",
        "decoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    decoder_input_seqs,\n",
        "    value=target_token2idx['<PAD>'],\n",
        "    padding='post',\n",
        "    maxlen=max_dec_seq_length)\n",
        "\n",
        "decoder_target_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    decoder_target_seqs,\n",
        "    value=target_token2idx['<PAD>'],\n",
        "    padding='post',\n",
        "    maxlen=max_dec_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6yK4dpuoC6D"
      },
      "source": [
        "### Создание модели\n",
        "\n",
        "Для создания Encoder-Decoder модели воспользуемся смесью двух стилей: реализация моделей через собственный класс и функциональный API. \n",
        "\n",
        "Сами по себе Encoder и Decoder (по отдельности) удобно реализовать в виде кастомных классов (наследованных от `tf.keras.Model`), так как у них может быть какая-то сложная реализация.\n",
        "\n",
        "В нашем случае Encoder будет состоять из двух LSTM слоей, который будет возвращать финальное состояние после прохода по всей цепочке. В качестве состояния нас интересует и вектор `h` и вектор состояния LSTM `c`. Для него нам понадобится дополнительный флаг `return_state=True`\n",
        "\n",
        "В Декодере будет два LSTM и полносвязный слой для генерации финальных ответов (распределение вероятностей по символам). Для прямого распространения (`__call__`) кроме входной цепочки декодер будет получать состояние от энкодера (`init_state`) и будет передавать его в свой LSTM слоя в качестве начального состояния, а возвращать будет предсказанную выходную цепочку (той же длины, return_sequences=True) состояние этого LSTM.\n",
        "\n",
        "После того, как мы отдельно построили Encoder и Decoder, надо соединить их в Encoder-Decoder. Но так как нам нужно создать несколько входов в модель (отдельно входная цепочка в энкодер, отдельно входная цепочка в декодер) очень удобно сделать это с помощью функционального API. Фходные узлы создаются с помощью `tf.keras.layers.Input`, а затем строим вычислительный граф, используя модели `encoder_model` и `decoder_model`.\n",
        "\n",
        "Используем разные `Embedding` слои для encoder и decoder, несмотря на то, что язык общения в диалогах один и тотже, для encoder'а и decoder'а они по факту разные. Также на входе и encoder и decoder ставим `Masking` слой для \"отрезания\" паддингов (чтобы обратное распространение градиентов по паддингам не считалось).\n",
        "\n",
        "Финальная модель -- `seq2seq`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F8eHmWeoC6D"
      },
      "source": [
        "H_SIZE = 2048 # Размерность скрытого состояния LSTM\n",
        "EMB_SIZE = 256 # размерность эмбеддингов (и для входных и для выходных цепочек)\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, is_bidirectional=False, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.masking = tf.keras.layers.Masking(mask_value=input_token2idx['<PAD>'])\n",
        "        self.embed = tf.keras.layers.Embedding(INPUT_VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.lstm2 = tf.keras.layers.LSTM(H_SIZE, return_sequences=False, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "\n",
        "        # bidirectional mode is sum to return that same shape of H, C as for single dir.\n",
        "        if self.is_bidirectional:\n",
        "            self.lstm1 = tf.keras.layers.Bidirectional(self.lstm1, merge_mode='sum')\n",
        "            self.lstm2 = tf.keras.layers.Bidirectional(self.lstm2, merge_mode='sum')\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.masking(x)\n",
        "        out = self.embed(out)\n",
        "\n",
        "        if self.is_bidirectional:\n",
        "            out, h_f, c_f, h_b, c_b = self.lstm1(out)\n",
        "            h1 = h_f + h_b\n",
        "            c1 = c_f + c_b\n",
        "            \n",
        "            _, h_f, c_f, h_b, c_b = self.lstm2(out)\n",
        "            h2 = h_f + h_b\n",
        "            c2 = c_f + c_b\n",
        "        else:\n",
        "            out, h1, c1 = self.lstm1(out)\n",
        "            _, h2, c2 = self.lstm2(out)\n",
        "\n",
        "        state = ((h1, c1), (h2, c2))\n",
        "        return state\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.masking = tf.keras.layers.Masking(mask_value=target_token2idx['<PAD>'])\n",
        "        self.embed = tf.keras.layers.Embedding(TARGET_VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.lstm2 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "        self.fc = tf.keras.layers.Dense(TARGET_VOCAB_SIZE, activation='softmax')\n",
        "        \n",
        "    def call(self, x, init_state):\n",
        "        out = self.masking(x)\n",
        "        out = self.embed(out)\n",
        "        out, h1, c1 = self.lstm1(out, initial_state=init_state[0])\n",
        "        out, h2, c2 = self.lstm2(out, initial_state=init_state[1])\n",
        "        out = self.fc(out)\n",
        "        state = ((h1, c1), (h2, c2))\n",
        "        return out, state\n",
        "\n",
        "\n",
        "encoder_model = Encoder(is_bidirectional=True)\n",
        "decoder_model = Decoder(dropout=0.4)\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "enc_state = encoder_model(encoder_inputs)\n",
        "decoder_outputs, _ = decoder_model(decoder_inputs, enc_state)\n",
        "\n",
        "seq2seq = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvyAeS5DoC6D",
        "outputId": "adb43495-760a-40b4-886f-a84a23d12434"
      },
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    root_folder + 'models/model-tokenRNN/checkpoint.hdf5',\n",
        "    monitor='loss', save_weights_only=True)\n",
        "\n",
        "seq2seq.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "seq2seq.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Encoder)               (((None, 2048), (Non 113523968   input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Decoder)               ((None, None, 34764) 132576204   input_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 246,100,172\n",
            "Trainable params: 246,100,172\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biokF2XioC6D"
      },
      "source": [
        "### Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS9Ws4xqoC6D"
      },
      "source": [
        "BATCH_SIZE = 54\n",
        "EPOCHS = 10\n",
        "1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R80JxwY1oC6D",
        "outputId": "b6301c34-447d-487c-de41-1ec4856bc613"
      },
      "source": [
        "# change to True to continue training from checkpoint\n",
        "# or save weights if checkpoint fails to do it\n",
        "restore_saved_model = True\n",
        "\n",
        "if restore_saved_model:\n",
        "    seq2seq.load_weights(\n",
        "        root_folder + 'models/model-tokenRNN/checkpoint.hdf5'\n",
        "        )\n",
        "    print('weights are restored')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights are restored\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De0gjAF3oC6E"
      },
      "source": [
        "EPOCHS = 6\n",
        "epochs_counter = 34"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "p5S41iDSoC6E",
        "outputId": "3d7a73e9-9e7e-4eb9-cbd3-bc97129381d1"
      },
      "source": [
        "# данных настолько много, что в эпохе 3000+ итераций (больше часа на 1 эпоху)\n",
        "steps_per_epoch = encoder_input_seqs.shape[0] // BATCH_SIZE\n",
        "\n",
        "seq2seq.fit(train_ds, epochs=EPOCHS + epochs_counter, \n",
        "            initial_epoch=epochs_counter, steps_per_epoch=steps_per_epoch, \n",
        "            callbacks=[checkpoint]) \n",
        "\n",
        "epochs_counter += EPOCHS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 35/40\n",
            "2624/2624 [==============================] - 2295s 875ms/step - loss: 0.6765 - accuracy: 0.8813\n",
            "Epoch 36/40\n",
            "2624/2624 [==============================] - 2290s 873ms/step - loss: 0.6672 - accuracy: 0.8833\n",
            "Epoch 37/40\n",
            "2624/2624 [==============================] - 2297s 875ms/step - loss: 0.6563 - accuracy: 0.8853\n",
            "Epoch 38/40\n",
            "2624/2624 [==============================] - 2329s 888ms/step - loss: 0.6473 - accuracy: 0.8872\n",
            "Epoch 39/40\n",
            "2624/2624 [==============================] - 2362s 900ms/step - loss: 0.6401 - accuracy: 0.8890\n",
            "Epoch 40/40\n",
            "2624/2624 [==============================] - 2347s 894ms/step - loss: 0.6382 - accuracy: 0.8897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmXfdNBGoC6E"
      },
      "source": [
        "# run it if the last epoch showed greater loss than previous checkpoint\n",
        "backup_model = True\n",
        "\n",
        "if backup_model:\n",
        "    seq2seq.save_weights(\n",
        "        root_folder + 'models/model-tokenRNN/checkpoint.hdf5'\n",
        "       )\n",
        "    print('Weights saved.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSdIsCixoC6E"
      },
      "source": [
        "### Функция для инференса\n",
        "\n",
        "Запуск инференса для Encoder-Decoder состоит из последовательного применения энкодера и декодера. \n",
        "\n",
        "Сначала прогоняем входную цепочку через энкодер и получаем закодированное представление `state`.\n",
        "\n",
        "А дальше применяем декодер в похожем режиме, как это было с генератором текста (только теперь передаём `state` в качестве начального состояния). В цикле постепенно генерируем выходную цепочку, подавая в декодер лишь один (текущий) токен и получая один предсказанный (следующий) токен. Начинаем с токена `<START>` и повторяем до тех пор, пока не получим `<END>` на выходе или не достигнем лимита по количеству токенов в цепочке. Для определения того, какой токен предсказал декодер, просто воспользуемся  функцией `argmax` для выходного распределения (выхода FC слоя)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j03ITc2woC6E"
      },
      "source": [
        "def mix_states(state1, state2, weights=None):\n",
        "    \"\"\"\n",
        "    Both states is expected to be a tuple of N tuples of pairs, where N is a \n",
        "    number of hidden RNN layers in encoder/decoder, and pairs are tuples of \n",
        "    hidden state (h, c) (for LSTM layer).\n",
        "    If hidden state doesn't have memory state (like GRU), then \n",
        "    state is expected to be like this: (h, )\n",
        "    default weights = [0.5, 0.5]. They're weights to mix state1 and state2 in\n",
        "    its weigthed average.\n",
        "    For example, two LSTM layers model should have state = ((h1, c1), (h2, c2))\n",
        "    where h1,c1 - state of the 1st LSTM layer, h2,c2 - state of 2nd LSTM layer.\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        weights = [0.5, 0.5]\n",
        "    dim1 = len(state1) # number of hidden layers\n",
        "    out = []\n",
        "    for i in range(dim1):\n",
        "        layer_state = []\n",
        "        for j in range(len(state1[i])):\n",
        "            layer_state.append(\n",
        "                weights[0] * state1[i][j] + weights[1] * state2[i][j]\n",
        "            )\n",
        "        out.append(tuple(layer_state))\n",
        "    return tuple(out)\n",
        "\n",
        "\n",
        "def generate_numbers(token):\n",
        "    if token == '<NUM>':\n",
        "        token = str(np.random.randint(100))\n",
        "    elif token == '<TIME>':\n",
        "        hours = np.random.randint(0, 12)\n",
        "        minutes = np.random.randint(0, 60)\n",
        "        token = f'{hours}:{minutes}'\n",
        "        pass\n",
        "    elif token == '<PRICE>':\n",
        "        token = '$' + str(np.random.randint(100))\n",
        "\n",
        "    return token\n",
        "\n",
        "def seq2seq_inference(input_seq, state_init=None, weights=None):\n",
        "    max_length = 40\n",
        "    input_seq = re.sub(r'[\\n]', '', input_seq) # delete \\n as it's not in vocab\n",
        "    input_seq = prepare_string(input_seq).split(' ')\n",
        "    input_seq = np.array([[input_token2idx.get(token, input_token2idx['<UNK>']) for token in input_seq]])\n",
        "    state = encoder_model(input_seq)\n",
        "    \n",
        "    if weights is None:\n",
        "        weights=[0.8, 0.2]\n",
        "        \n",
        "    if state_init is not None:\n",
        "        state = mix_states(state, state_init, weights=[0.8, 0.2])\n",
        "\n",
        "    target_seq = np.array([[target_token2idx['<START>']]])\n",
        "\n",
        "    decoded_sentence = []\n",
        "    while True:\n",
        "        output_tokens, state = decoder_model(target_seq, state)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = target_idx2token[sampled_token_index]\n",
        "\n",
        "        if (sampled_token == '<END>' or\n",
        "           len(decoded_sentence) > max_length):\n",
        "            break\n",
        "        sampled_token = generate_numbers(sampled_token)\n",
        "        decoded_sentence.append(sampled_token)\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "    \n",
        "    s = \" \".join(decoded_sentence) # join tokens in a string\n",
        "    s = re.sub(r\"\\s([?.!,-])\", r\"\\1\", s).strip() # remove spaces before puctuation\n",
        "    \n",
        "    return s, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pvi_QEEpoC6F",
        "outputId": "4cb0ff04-adc1-497f-b363-0191cefd82d5"
      },
      "source": [
        "decoded_sentence, state = seq2seq_inference(\"How do you feel?\")\n",
        "print(decoded_sentence)\n",
        "decoded_sentence, state = seq2seq_inference(\"Good afternoon, mister.\")\n",
        "print(decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i don't know. what do we have to do?\n",
            "oh my god, sir!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp263AoRoC6F",
        "outputId": "d4e4cb6c-9f53-473e-8f58-b01d3df256c5"
      },
      "source": [
        "# two bots talking...\n",
        "\n",
        "state = None\n",
        "weights = [0.9, 0.1]\n",
        "\n",
        "dialog = [\"Tell me a joke.\"]\n",
        "print(f'User: {dialog[0]}')\n",
        "for seq_index in range(0, 4):\n",
        "    length = len(dialog)\n",
        "    \n",
        "    decoded_sentence, state = seq2seq_inference(\" \".join(dialog[-min(4, length):]), state, weights)\n",
        "    print(f'Bot1: {decoded_sentence}')\n",
        "    dialog.append(decoded_sentence)\n",
        "    length += 1\n",
        "    \n",
        "    decoded_sentence, state = seq2seq_inference(\" \".join(dialog[-min(4, length):]), state, weights)\n",
        "    print(f'Bot2: {decoded_sentence}')\n",
        "    dialog.append(decoded_sentence)\n",
        "    length += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User: Tell me a joke.\n",
            "Bot1: there are so many questions.\n",
            "Bot2: who are they? what do you expect from me-\n",
            "Bot1: dear nothing in the world you haven't been before. you were saying anything morning--\n",
            "Bot2: yes three- thirty. i used to love this little man.\n",
            "Bot1: seven--\n",
            "Bot2: bullshit.\n",
            "Bot1: it's been over. have to get back to the hotel.\n",
            "Bot2: maybe two million dollars for each?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOW6sZe2oC6F",
        "outputId": "86fc2bc9-82c8-470b-ef31-9fc9b93b14bc"
      },
      "source": [
        "# chat with bot...\n",
        "\n",
        "state = None\n",
        "weights = [1, 0]\n",
        "\n",
        "dialog = []\n",
        "length = 0\n",
        "for seq_index in range(0, 5):\n",
        "    \n",
        "    message = input('User: ')\n",
        "    dialog.append(message)\n",
        "    length += 1\n",
        "    \n",
        "    message, state = seq2seq_inference(\" \".join(dialog[-min(4, length):]), state, weights)\n",
        "    print(f'Bot1: {message}')\n",
        "    dialog.append(message)\n",
        "    length += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User: Hello there. My name is Tom. Who are you?\n",
            "Bot1: it's my first friend.\n",
            "User: What is your name?\n",
            "Bot1: admiral.\n",
            "User: Where are you from?\n",
            "Bot1: , harry.\n",
            "User: How old are you?\n",
            "Bot1: i'm just trying to help.\n",
            "User: Are you OK?\n",
            "Bot1: i am, too.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9whDuuLoC6F"
      },
      "source": [
        "After 34 epochs (about 140-150k training steps, I had to reduce batch_size from 64 to 60 and then to 54 due to memory contrains):\n",
        "\n",
        "User: Why can't we be friends.<br>\n",
        "Bot1: why?<br>\n",
        "Bot2: i know all about it.<br>\n",
        "Bot1: ... well, you know, you know, i know, i mean, what do you want?<br>\n",
        "Bot2: like to know what i'm talking about.<br>\n",
        "Bot1: huh?<br>\n",
        "Bot2: no, i don't.<br>\n",
        "Bot1: huh... sure.<br>\n",
        "Bot2: get a drink. just tell them if you're right.<br>\n",
        "\n",
        "User: Hello there. My name is Tom. Who are you?<br>\n",
        "Bot1: it's me.<br>\n",
        "User: your name?<br>\n",
        "Bot1: ahh... do you want to come in?<br>\n",
        "User: I'm already here. Who are you?<br>\n",
        "Bot1: i came here to shut up.<br>\n",
        "User: ha-ha-ha. So go ahead and shut up!<br>\n",
        "Bot1: of course. there is one thing-- but there is no whole dead.<br>\n",
        "User: you scary me...<br>\n",
        "Bot1: inside? with what? there's nothing to talk about.\n",
        "\n",
        "After 40 epochs:\n",
        "\n",
        "User: Why can't we be friends?<br>\n",
        "Bot1: i don't know.<br>\n",
        "Bot2: i don't think that's a good idea.<br>\n",
        "Bot1: bullshit.<br>\n",
        "Bot2: holy. what do you mean?<br>\n",
        "Bot1: put me down.<br>\n",
        "Bot2: you do?<br>\n",
        "Bot1: what?<br>\n",
        "Bot2: what do you mean?<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOSrzSXyoC6F"
      },
      "source": [
        "## Вариант 3. Token-RNN Encoder-Decoder with Attention.\n",
        "\n",
        "Вся подготовка данных, включая паддинг, аналогична Варианту 2. Различия начинаются с модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z96KO_C-oC6F"
      },
      "source": [
        "### Подготовка данных: input-target\n",
        "\n",
        "Создадим списки из входных данных и соответствующих им целевых последовательностей.\n",
        "Есть два варианта определения вход-выход:\n",
        "1. Перебирая целевые высказывания от 2-го до последнего в диалоге, в качестве последовательности на входе берем предыдущее высказывание. Например, если далог состоит из 4 высказываний: `[a, b, c, d]` то пары input(I)-taget(T) будут:\n",
        "  - I: `a`, T: `b`\n",
        "  - I: `b`, T: `c`\n",
        "  - I: `c`, T: `d`\n",
        "2. Перебирая целевые высказывания от 2-го до последнего в диалоге, в качестве последовательности на входе берем все высказывания, идущие перед целевым.\n",
        "Например, если далог состоит из 4 высказываний: `[a, b, c, d]` то пары input(I)-taget(T) будут:\n",
        "  - I: `a`, T: `b`\n",
        "  - I: `a b`, T: `c`\n",
        "  - I: `a b c`, T: `d`\n",
        "\n",
        "Для большей глубины мысли в encoder'е выберем 2-й вариант. Но при этом не будем использовать двунаправленную RNN чтоб важность последнего высказывания имело больший вес (как оно и будет происходить в однонаправленной сети).\n",
        "\n",
        "Каждое высказывание подготавливаем для токенизации: числа заменяем на `<NUM>`, время на `<TIME>`, стоимость на `<PRICE>`, удаляем не пунктуационные знаки `&gt; &lt; &quot; * ~ | { } _ [ ]`, удаляем лишние пробелы.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHFNo2yyoC6G",
        "outputId": "4e20c1dd-f762-417a-b285-a11a91f2b5e7"
      },
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "max_len_input, max_len_target = 0, 0\n",
        "MAX_SEQ_LEN = 15\n",
        "\n",
        "def prepare_string(s):\n",
        "    \"\"\"\n",
        "    Prepair utterance for tokenization.\n",
        "    \"\"\"\n",
        "    # separate punctuation\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"([?.!,-])\", r\" \\1 \", s)\n",
        "    # remove special characters\n",
        "    s = re.sub(r\"(&gt;)|(&lt;)|(&quot;)|([\\*~|{}_\\[\\]`])\", '', s)\n",
        "    # substitute '&' to 'and'\n",
        "    s = re.sub(r'\\s&\\s', ' and ', s)\n",
        "    s = re.sub(r\"(?<=\\s)'(?=[a-z])|^'(?=[a-z])\", \"\", s) # remove ' at the beginning of the word\n",
        "    s = re.sub(r\"(<[A-Za-z]+>)|(</[A-Za-z]+>)\", \"\", s) # remove html-style tags\n",
        "    s = re.sub(r\"(^\\$[0-9]+)|([0-9]+\\$)\", ' <PRICE> ', s) # replace prices with <PRICE> like $5 or 10$\n",
        "    s = re.sub(r'([0-9]{1,2}h)|([0-9]{1,2}:[0-9]{2})', ' <TIME> ', s) # time formats like 5h, 10:35 to <TIME>\n",
        "    s = re.sub(r\"(?<=[0-9])(['%])\", r\" \\g<1>\", s) # separate % and ' from numbers, like \"007's\" or \"10%\"\n",
        "    s = re.sub(r\"[0-9]+-?((th)|(nd)|(st)|(rd))\", \" <NUM> \", s) # 1st, 2nd, 3rd, 10th or 10-th replace with <NUM>\n",
        "    s = re.sub(r\"(?<=[0-9])([A-Za-z]+)\", r' \\g<1>', s) # separate units like 33mm 300W\n",
        "    s = re.sub(r\"(#[0-9]+)|([0-9]+)\", ' <NUM> ', s) # replace numbers with <NUM>\n",
        "    # repalace double spaces and \\t with single space\n",
        "    s = re.sub(r'[\" \"\\t]+', \" \", s).strip()\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "for dialog in tqdm(dialogs):\n",
        "    for i in range(1, len(dialog)):\n",
        "        # join (1 + ... + i-1) utterances in one input\n",
        "        input_text = ' '.join([utterances[dialog[j]] for j in range(i)])\n",
        "        # condition text\n",
        "        input_text = prepare_string(input_text).split(' ')\n",
        "        # assign i-th utterance as target and condition it\n",
        "        target_text = prepare_string(utterances[dialog[i]]).split(' ')\n",
        "\n",
        "        # stop processing this dialog if input or target max length is reached\n",
        "        if max(len(input_text), len(target_text)) > MAX_SEQ_LEN:\n",
        "            break\n",
        "        \n",
        "        max_len_input = max(max_len_input, len(input_text))\n",
        "        max_len_target = max(max_len_target, len(target_text))\n",
        "\n",
        "        # it was noticed that some utterances are empty, for example L474\n",
        "        if (len(target_text) == 1) and (target_text[0] == ''):\n",
        "            # skip empty lines\n",
        "            continue\n",
        "        if (len(input_text) == 1) and (input_text[0] == ''):\n",
        "            # skip empty lines\n",
        "            continue\n",
        "\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "\n",
        "print('Length of input/target lists:', len(input_texts))\n",
        "print('Max length of input:', max_len_input)\n",
        "print('Max length of target:', max_len_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 83097/83097 [00:08<00:00, 10122.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Length of input/target lists: 61141\n",
            "Max length of input: 15\n",
            "Max length of target: 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGwNL3wyoC6G"
      },
      "source": [
        "Проведем небольшой анализ объема тренировочных данных с разной максимальной длиной последовательности токенов:\n",
        "- MAX_SEQ_LEN=100 -> 196 239 samples\n",
        "- MAX_SEQ_LEN=50 -> 158 139 samples\n",
        "- MAX_SEQ_LEN=40 -> 141 700 samples\n",
        "- MAX_SEQ_LEN=30 -> 118 426 samples\n",
        "- MAX_SEQ_LEN=25 -> 102 967 samples\n",
        "- MAX_SEQ_LEN=20 -> 84 140 samples\n",
        "- MAX_SEQ_LEN=15 -> 61 141 samples\n",
        "\n",
        "Чтобы избежать слишком большого объема тренировочных данных, ограничим максимальное количество токенов с высказываниях 40-ю. При этом количество диалогов чуть превышает объем тренировочных данных, используемых в Char-RNN, но это оправдано гораздо большим количеством различных токенов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZNalbnxoC6G",
        "outputId": "26500680-8fc6-48cc-fcb2-8107adf36c43"
      },
      "source": [
        "# random sample from training set\n",
        "i = np.random.randint(0, high=len(input_texts))\n",
        "print(f'Sample #{i}:')\n",
        "print('-', ' '.join(input_texts[i]))\n",
        "print('-', ' '.join(target_texts[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample #24585:\n",
            "- aye , he looks a right lurker .\n",
            "- you're undressed . where are your clothes ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdvsO9nPoC6G"
      },
      "source": [
        "### Подготовка словаря\n",
        "\n",
        "Одно слово или знак препинания - один токен.\n",
        "\n",
        "Для входа (input) и выхода (target) подготовим два словаря (отображения индекса в токен и токен в индекс), и сделаем это для всех высказований из `input_texts` и `target_texts`.\n",
        "\n",
        "Кроме того, нам понадобятся специальные токены для начала и конца цепочки, и паддинга (`<START>`, `<END>`, `<PAD>`). Также из input_texts уберем нетекстовые токены (начинающиеся не с буквы, например: цифры, скобки, ковычки, теги и пр.), заменив их на `<UNK>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlXaflHIoC6G",
        "outputId": "71bb7996-b7c7-447d-84bd-7ce16f3e7d0f"
      },
      "source": [
        "def prepare_vocab(texts, is_input=False):\n",
        "    vocab = set()\n",
        "    counter = Counter()\n",
        "    for text in texts:\n",
        "        # for input_texts with replace irrelevant tokens with <UNK>\n",
        "        # token should start with letter or basic punctuation ?.,!-\n",
        "        # if is_input:\n",
        "        #     text = [s if re.match(r'^[A-Za-z?.,!-]', s) else '<UNK>'  for s in text]\n",
        "        vocab.update(set(text))\n",
        "        for token in text:\n",
        "            counter[token] += 1\n",
        "    # для target датасета добавим теги начала и конца последовательности\n",
        "    if not is_input:\n",
        "        vocab.add('<START>')\n",
        "        vocab.add('<END>')\n",
        "    \n",
        "    vocab.add('<PAD>')\n",
        "    vocab.add('<UNK>')\n",
        "    vocab = sorted(vocab)\n",
        "    vocab_size = len(vocab)\n",
        "    token2idx = {u:i for i, u in enumerate(vocab)}\n",
        "    idx2token = np.array(vocab)\n",
        "    return vocab_size, token2idx, idx2token, counter\n",
        "\n",
        "\n",
        "INPUT_VOCAB_SIZE, input_token2idx, input_idx2token, input_counter = prepare_vocab(input_texts, True)\n",
        "TARGET_VOCAB_SIZE, target_token2idx, target_idx2token, target_counter = prepare_vocab(target_texts)\n",
        "\n",
        "print(f'Input vocab size: {INPUT_VOCAB_SIZE}')\n",
        "for i in range(2):\n",
        "    rare_tokens = sum([1 for freq in input_counter.values() if freq == i+1])\n",
        "    text = f'  Tokens with frequency {i+1}:  {rare_tokens} ({rare_tokens/INPUT_VOCAB_SIZE*100:.2f}%)'\n",
        "    print(text)\n",
        "print(f'Target vocab size: {TARGET_VOCAB_SIZE}')\n",
        "for i in range(2):\n",
        "    rare_tokens = sum([1 for freq in target_counter.values() if freq == i+1])\n",
        "    text = f'  Tokens with frequency {i+1}:  {rare_tokens} ({rare_tokens/TARGET_VOCAB_SIZE*100:.2f}%)'\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input vocab size: 17113\n",
            "  Tokens with frequency 1:  7363 (43.03%)\n",
            "  Tokens with frequency 2:  2969 (17.35%)\n",
            "Target vocab size: 17239\n",
            "  Tokens with frequency 1:  8806 (51.08%)\n",
            "  Tokens with frequency 2:  2661 (15.44%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvN5LVlSoC6H"
      },
      "source": [
        "del input_counter\n",
        "del target_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHgVyQKsoC6H"
      },
      "source": [
        "В полном наборе данных было 85к уникальных токенов. В данной выборке количество одноразовых токенов хоть и снизилось (в том числе из-за подготовки текста), но их процент остался на прежнем уровне (около 50% на target_texts)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfrcUJiooC6H"
      },
      "source": [
        "### Подготовка обучающего датасета\n",
        "\n",
        "Наша модель будет состоять из двух частей: `Encoder` и `Decoder`. Задача энкодера считать входную цепочку и получить её закодированное представление. А задача декодера по этому закодированному представлению получить выходную цепочку.\n",
        "\n",
        "Декодер по сути является генератором текста, поэтому используется он аналогично тому, как мы это делали ранее с символьным генератором текста. Отличие только в том, что тут декодер будет получать начальное состояние из энкодера, а в качестве \"начала\" цепочки будет получать токен `<START>`.\n",
        "\n",
        "И точно так же, как и в случае с генератором, для обучения декодера в качестве входа и целевого выхода будем использовать одну и ту же цепочку, но сдвинутую на один элемент во времени. В конце Декодер должен предсказать токен `<END>`.\n",
        "\n",
        "Например, входом и выходом для декодера могут быть такие две цепочки из семи символов (начальный и конечный токен это один символ):\n",
        "\n",
        "`<START>Привет` --> `Привет<END>`\n",
        "\n",
        "Таким образом, для **обучения** `Encoder-Decoder` нам понадобятся три набора цепочек:\n",
        " - `encoder_input_seqs` - входы в Encoder\n",
        " - `decoder_input_seqs` - входы в Decoder\n",
        " - `decoder_target_seqs` - целевые выходы из Decoder (и всей модели Encoder-Decoder)\n",
        "\n",
        "Сами цепочки будут являться последовательностями целочисленных индексов (полученных с помощью соответствующих словарей)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3T8iuizoC6H",
        "outputId": "b5b8d881-5a1c-4d9f-feae-99aefc5750ae"
      },
      "source": [
        "%%time\n",
        "input_texts_as_int = [[input_token2idx.get(token, input_token2idx['<UNK>']) for token in text] for text in input_texts]\n",
        "target_texts_as_int = [[target_token2idx.get(token, target_token2idx['<UNK>']) for token in text] for text in target_texts]\n",
        "\n",
        "encoder_input_seqs = [np.array(text) for text in input_texts_as_int]\n",
        "decoder_input_seqs = []\n",
        "decoder_target_seqs = []\n",
        "for target_text in target_texts_as_int:\n",
        "    decoder_input_seqs.append(np.array([target_token2idx['<START>']] + target_text))\n",
        "    decoder_target_seqs.append(np.array(target_text + [target_token2idx['<END>']]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 508 ms, sys: 12 ms, total: 520 ms\n",
            "Wall time: 518 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn2HJ7M3oC6H"
      },
      "source": [
        "### Паддинг цепочек\n",
        "\n",
        "Вспомним, что для обучения нам надо использовать батчи, которые состоят из цепочек одинаковой длины. А изначально длина цепочек (как входных, так и выходных) может быть произвольной. Поэтому нам необходимо сделать паддинг -- дополнить все цепочки до некоторой фиксированной длины с помощью символа `<PAD>`. В качестве длин будем брать фактическую максимальную длину последовательностей (отдельно для входных, отдельно для выходных). Напомню, что для тренировочных данных мы брали цепочки не блинее 500 символов. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf6mM98woC6H",
        "outputId": "f7b521e9-d20d-4ebd-f914-66133417d327"
      },
      "source": [
        "max_enc_seq_length = max([len(seq) for seq in encoder_input_seqs])\n",
        "max_dec_seq_length = max([len(seq) for seq in decoder_input_seqs])\n",
        "\n",
        "encoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    encoder_input_seqs,\n",
        "    value=input_token2idx['<PAD>'],\n",
        "    padding='post',\n",
        "    maxlen=max_enc_seq_length)\n",
        "\n",
        "# decoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "#     decoder_input_seqs,\n",
        "#     value=target_token2idx['<PAD>'],\n",
        "#     padding='post',\n",
        "#     maxlen=max_dec_seq_length)\n",
        "\n",
        "decoder_target_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    decoder_target_seqs,\n",
        "    value=target_token2idx['<PAD>'],\n",
        "    padding='post',\n",
        "    maxlen=max_dec_seq_length)\n",
        "\n",
        "encoder_input_seqs.shape,  decoder_target_seqs.shape # decoder_input_seqs.shape,"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((61141, 15), (61141, 16))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxOL-cuCoC6I"
      },
      "source": [
        "### Создание модели\n",
        "\n",
        "Для создания Encoder-Decoder модели воспользуемся тремя классами: Encoder, Decoder (наследуемые от `tf.keras.Model`) и Attention (наследуемый от `tf.keras.layers.Layer`). \n",
        "\n",
        "Сами по себе Encoder и Decoder (по отдельности) удобно реализовать в виде кастомных классов (наследованных от `tf.keras.Model`), так как у них может быть какая-то сложная реализация.\n",
        "\n",
        "В нашем случае Encoder будет состоять из двух LSTM слоей, который будет возвращать финальное состояние после прохода по всей цепочке. В качестве состояния нас интересует и вектор `h` и вектор состояния LSTM `c`. Для него нам понадобится дополнительный флаг `return_state=True`. В отличие от предыдущих моделей, будем возвращать также и полную цепочку с последнего LSTM слоя (return_sequences=True) для механизма Attention.\n",
        "\n",
        "В Декодере будет два LSTM и полносвязный слой для генерации финальных ответов (распределение вероятностей по символам). \n",
        "\n",
        "В отличии от модели без Attention, предсказания будут делаться только по одному токену за раз, так как на вход слоя с Attention поступают выход енкодера и текущее состояние (будем использовать состояние `h` последнего LSTM слоя), которое изменяется после каждого предсказанного токена декодером. Важно отметить, что Attention содержит Dense слои, следовательно длина входной цепочки всегда **фиксирована** с момента инициализации (в нашем случае 40 токенов).\n",
        "\n",
        "Для прямого распространения декодера (`__call__`) кроме входной цепочки декодер будет получать выхоную цепочку с енкодера, состояния слоев от энкодера (`init_state`) и будет передавать его в свои LSTM слои в качестве начального состояния, а возвращать будет предсказанный токен и новые состояния LSTM слоев. \n",
        "\n",
        "Используем разные `Embedding` слои для encoder и decoder, несмотря на то, что язык общения в диалогах один и тотже, для encoder'а и decoder'а они по факту разные. Также на входе encoder ставим `Masking` слой для \"отрезания\" паддингов (чтобы обратное распространение градиентов по паддингам не считалось). Но для decoder паддинг токены будем обрабатывать вручную (зануляя loss если ground truth token является паддингом).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETt0kmZUoC6I"
      },
      "source": [
        "H_SIZE = 2048 # Размерность скрытого состояния LSTM\n",
        "EMB_SIZE = 256 # размерность эмбеддингов (и для входных и для выходных цепочек)\n",
        "ATT_UNITS = 2048 # attention units\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, is_bidirectional=False, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.masking = tf.keras.layers.Masking(mask_value=input_token2idx['<PAD>'])\n",
        "        self.embed = tf.keras.layers.Embedding(INPUT_VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "#         self.lstm2 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "#                                           return_state=True, dropout=dropout)\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "\n",
        "        # bidirectional mode is sum to return that same shape of H, C as for single dir.\n",
        "        if self.is_bidirectional:\n",
        "            self.lstm1 = tf.keras.layers.Bidirectional(self.lstm1, merge_mode='sum')\n",
        "#             self.lstm2 = tf.keras.layers.Bidirectional(self.lstm2, merge_mode='sum')\n",
        "        \n",
        "    def call(self, x, init_state): # initial_state = hidden\n",
        "        out = self.masking(x)\n",
        "        out = self.embed(out)\n",
        "\n",
        "        if self.is_bidirectional:\n",
        "            out, h_f, c_f, h_b, c_b = self.lstm1(out, initial_state=init_state[0])\n",
        "            h1 = h_f + h_b\n",
        "            c1 = c_f + c_b\n",
        "            \n",
        "#             out, h_f, c_f, h_b, c_b = self.lstm2(out, initial_state=init_state[1])\n",
        "#             h2 = h_f + h_b\n",
        "#             c2 = c_f + c_b\n",
        "        else:\n",
        "            out, h1, c1 = self.lstm1(out, initial_state=init_state[0])\n",
        "#             out, h2, c2 = self.lstm2(out, initial_state=init_state[1])\n",
        "\n",
        "        state = ((h1, c1), ) #(h2, c2))\n",
        "        return out, state\n",
        "    \n",
        "    def initialize_hidden_state(self, batch_sz, enc_units=H_SIZE):\n",
        "        if self.is_bidirectional:\n",
        "            # ((h1_f, c1_f, h1_b, c1_b),(h2_f, c2_f, h2_b, c2_b))\n",
        "            return ((tf.zeros((batch_sz, enc_units)),\n",
        "                    tf.zeros((batch_sz, enc_units)),\n",
        "                    tf.zeros((batch_sz, enc_units)),\n",
        "                    tf.zeros((batch_sz, enc_units))),)\n",
        "#                    (tf.zeros((batch_sz, enc_units)),\n",
        "#                     tf.zeros((batch_sz, enc_units)),\n",
        "#                     tf.zeros((batch_sz, enc_units)),\n",
        "#                     tf.zeros((batch_sz, enc_units))))\n",
        "        else:\n",
        "            # ((h1, c1), (h2, c2))\n",
        "            return ((tf.zeros((batch_sz, enc_units)),\n",
        "                    tf.zeros((batch_sz, enc_units))), )\n",
        "#                   (tf.zeros((batch_sz, enc_units)),\n",
        "#                     tf.zeros((batch_sz, enc_units))))\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query hidden state (S_j-1) shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape (E_i) == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, attention_units, dropout=0.0):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.attention_units = attention_units\n",
        "        self.masking = tf.keras.layers.Masking(mask_value=target_token2idx['<PAD>'])\n",
        "        self.embed = tf.keras.layers.Embedding(TARGET_VOCAB_SIZE, EMB_SIZE)\n",
        "        self.lstm1 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "                                          return_state=True, dropout=dropout)\n",
        "#         self.lstm2 = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, \n",
        "#                                           return_state=True, dropout=dropout)\n",
        "        self.fc = tf.keras.layers.Dense(TARGET_VOCAB_SIZE) # without activation\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.attention_units)\n",
        "\n",
        "    def call(self, x, init_state, enc_output):\n",
        "        \"\"\"\n",
        "        Decoder with attention has to process 1 token at a time, so\n",
        "        x.shape = (batch_size, 1, 1), i.e. 1 seq_len, 1 token's code\n",
        "        \"\"\"\n",
        "\n",
        "        hidden = init_state[-1][0] # use last LSTM h-state for attention (e.g. h2)\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.masking(x)\n",
        "        x = self.embed(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to LSTM layers\n",
        "        out, h1, c1 = self.lstm1(x, initial_state=init_state[0])\n",
        "#         out, h2, c2 = self.lstm2(out, initial_state=init_state[1])\n",
        "\n",
        "        # out shape == (batch_size * 1, hidden_size)\n",
        "        out = tf.reshape(out, (-1, out.shape[2]))\n",
        "\n",
        "        # out shape == (batch_size, vocab)\n",
        "        out = self.fc(out)\n",
        "        state = ((h1, c1), ) # (h2, c2))\n",
        "        return out, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL_aOwF9oC6I"
      },
      "source": [
        "Напишем свою функцию потерь. В качестве основы возмем SparseCategoricalCrossentropy (из логитов и без нормирования loss'а по батчам - сделаем это в цикле обучения). Дополняем функцию потерь маской, зануляя loss для паддинг элеменов (из ground truth)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_B2cKRjoC6I"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none') # reduction done manually\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # mask to kill loss from <PAD> tokens in target ground truth\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, target_token2idx['<PAD>']))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-R9kh-XoC6I"
      },
      "source": [
        "Создадим экземпляры классов, заодно проверим размерности всех входов/выходов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ttSB8AcoC6J",
        "outputId": "79d004ce-1125-4619-916a-235de26f61f9"
      },
      "source": [
        "# use one batch of data as a sample during model creation\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((encoder_input_seqs, decoder_target_seqs))\n",
        "dataset = dataset.shuffle(encoder_input_seqs.shape[0]).batch(BATCH_SIZE)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "print('Data sample shapes: input {}, target {}\\n'.format(example_input_batch.shape, example_target_batch.shape))\n",
        "\n",
        "# create instance of encoder\n",
        "encoder = Encoder(is_bidirectional=True)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state(batch_sz=BATCH_SIZE) # with default batch_size and h_size\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape (last LSTM): (batch size, units) {}'.format(sample_hidden[-1][0].shape))\n",
        "print ('Encoder Hidden memory shape (last LSTM): (batch size, units) {}\\n'.format(sample_hidden[-1][1].shape))\n",
        "\n",
        "# create instance of Attention layer just to check its functionality\n",
        "attention_layer = BahdanauAttention(ATT_UNITS)\n",
        "# hidden state is h output of the last iteration on an input sequence from the second LSTM layer (e.g.: h2)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden[-1][0], sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, hidden_size) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\\n\".format(attention_weights.shape))\n",
        "\n",
        "# create instance of decoder\n",
        "decoder = Decoder(attention_units=ATT_UNITS, dropout=0.4)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data sample shapes: input (64, 15), target (64, 16)\n",
            "\n",
            "Encoder output shape: (batch size, sequence length, units) (64, 15, 2048)\n",
            "Encoder Hidden state shape (last LSTM): (batch size, units) (64, 2048)\n",
            "Encoder Hidden memory shape (last LSTM): (batch size, units) (64, 2048)\n",
            "\n",
            "Attention result shape: (batch size, hidden_size) (64, 2048)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 15, 1)\n",
            "\n",
            "Decoder output shape: (batch_size, vocab size) (64, 17239)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMlRZ72koC6J",
        "outputId": "a16e8907-ed80-4da9-98b5-43df6bf0f29a"
      },
      "source": [
        "encoder.summary()\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking (Masking)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        multiple                  4380928   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional multiple                  37765120  \n",
            "=================================================================\n",
            "Total params: 42,146,048\n",
            "Trainable params: 42,146,048\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "masking_1 (Masking)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      multiple                  4413184   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  35659776  \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  35322711  \n",
            "_________________________________________________________________\n",
            "bahdanau_attention_1 (Bahdan multiple                  8394753   \n",
            "=================================================================\n",
            "Total params: 83,790,424\n",
            "Trainable params: 83,790,424\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCkxdZx6oC6J"
      },
      "source": [
        "Checkpoint (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrIsODPWoC6J"
      },
      "source": [
        "del attention_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD5cmboroC6J"
      },
      "source": [
        "# extra imports\n",
        "import os\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# checkpoint\n",
        "checkpoint_dir = root_folder + 'models/model-tokenRNN-Attention/'\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=1, checkpoint_name='ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdtL-OzOoC6J"
      },
      "source": [
        "### Обучение модели\n",
        "\n",
        "Training steps ([tensorflow tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention)):\n",
        "1. Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
        "3. The decoder returns the predictions and the decoder hidden state.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use teacher forcing to decide the next input to the decoder.\n",
        "6. Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZxcoqDqoC6K"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden, metric):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([target_token2idx['<START>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(0, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # mask rows where target is <PAD> to exclude it from metrics\n",
        "            mask = tf.math.logical_not(tf.math.equal(targ[:, t], target_token2idx['<PAD>']))\n",
        "            metric.update_state(\n",
        "                tf.boolean_mask(targ[:, t], mask), \n",
        "                tf.boolean_mask(predictions, mask)\n",
        "            )\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cUIKMgXoC6K"
      },
      "source": [
        "epochs_counter = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEn9qMa-oC6K"
      },
      "source": [
        "Основной цикл обучения модели. Для наглядности реализуем схожее отображение прогресса обучения, как у функции fit()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNTBYfRFoC6K",
        "outputId": "9a00b122-d881-4c0a-fc56-4f52c2e814bf"
      },
      "source": [
        "EPOCHS = 20\n",
        "steps_per_epoch = len(encoder_input_seqs)//BATCH_SIZE\n",
        "\n",
        "# inits for progress bar\n",
        "MAX_OUT = 30\n",
        "epoch_digits = int(np.log10(steps_per_epoch)) + 1\n",
        "buffer = []\n",
        "\n",
        "\n",
        "def print_buffer(buffer):\n",
        "    \"\"\"Reprint buffer (static text of previous lines).\"\"\"\n",
        "    clear_output(wait=True)\n",
        "    for line in buffer:\n",
        "        print(line, end='')\n",
        "\n",
        "\n",
        "def progressbar_text(iteration, loss, time_passed, accuracy=None):\n",
        "    \"\"\"Print progress bar of current epoch with some statistics \n",
        "    like loss and ETA.\"\"\"\n",
        "    text = []\n",
        "    if iteration == steps_per_epoch:\n",
        "        # end of epoch text\n",
        "        progress_bar = '=' * MAX_OUT\n",
        "        iter_time_avg = time_passed / steps_per_epoch\n",
        "        if iter_time_avg > 1:\n",
        "            template = '{:.0f}s {:.0f}s/step'\n",
        "        else:\n",
        "            template = '{:.0f}s {:.0f}ms/step'\n",
        "            iter_time_avg *= 1000\n",
        "        text_time = template.format(time_passed, iter_time_avg)\n",
        "    else:\n",
        "        # in progress text\n",
        "        frac = int(np.round((iteration/steps_per_epoch) * MAX_OUT))\n",
        "        progress_bar = ('=' * (frac - 1)) + '>'  + '.' * (MAX_OUT - max(frac, 1))\n",
        "        iter_time_avg = time_passed / (iteration + 1)\n",
        "        est_time = int((steps_per_epoch - iteration - 1) * iter_time_avg)\n",
        "        text_time = 'ETA: {}:{:0>2d}'.format(est_time // 60, est_time % 60)\n",
        "    # prepare printout template with propper padding for itteration field\n",
        "    template = '{' + f':>{epoch_digits}d' + '}/{} [{}]'\n",
        "    text.append(template.format(iteration, steps_per_epoch, progress_bar))\n",
        "    text.append(text_time)\n",
        "    text.append('loss: {:.4f}'.format(loss))\n",
        "    if accuracy is not None:\n",
        "        text.append(f'accuracy: {float(accuracy):.4f}')\n",
        "    return ' - '.join(text)\n",
        "\n",
        "\n",
        "# main training cycle\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "metric_result = None\n",
        "\n",
        "for epoch in range(epochs_counter, EPOCHS+epochs_counter):  \n",
        "    buffer.append(f'Epoch {epoch+1}/{EPOCHS+epochs_counter}\\n') # stdout\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state(batch_sz=BATCH_SIZE)\n",
        "    total_loss = 0\n",
        "\n",
        "    start = time.time()\n",
        "    previous_time = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        # batch training\n",
        "        batch_loss = train_step(inp, targ, enc_hidden, metric)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        # print progress-bar not more frequently than 1 sec\n",
        "        # to limit print/clear load if steps are too fast\n",
        "        time_passed = time.time() - start\n",
        "        if time_passed - previous_time > 1:\n",
        "            previous_time = time_passed\n",
        "            print_buffer(buffer)\n",
        "            print(\n",
        "                progressbar_text(batch, total_loss.numpy()/(batch+1), \n",
        "                                 time_passed, metric_result)\n",
        "            )\n",
        "        \n",
        "        # refresh metric result every 20 iterations\n",
        "        if (batch + 1) % 20 == 0:\n",
        "            metric_result = metric.result()\n",
        "    \n",
        "    # save checkpoint\n",
        "    manager.save()\n",
        "    \n",
        "    # append final epoch statistics to buffer\n",
        "    buffer.append(\n",
        "        progressbar_text(steps_per_epoch, total_loss / steps_per_epoch, \n",
        "                         time_passed, metric.result()) + '\\n'\n",
        "        )\n",
        "    \n",
        "    # reset metric at the end of epoch\n",
        "    metric.reset_states()\n",
        "    metric_result = None\n",
        "\n",
        "print_buffer(buffer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/40\n",
            "955/955 [==============================] - 458s 479ms/step - loss: 0.2489 - accuracy: 0.8961\n",
            "Epoch 22/40\n",
            "955/955 [==============================] - 450s 471ms/step - loss: 0.2088 - accuracy: 0.9129\n",
            "Epoch 23/40\n",
            "955/955 [==============================] - 450s 471ms/step - loss: 0.1745 - accuracy: 0.9271\n",
            "Epoch 24/40\n",
            "955/955 [==============================] - 449s 470ms/step - loss: 0.1456 - accuracy: 0.9393\n",
            "Epoch 25/40\n",
            "955/955 [==============================] - 448s 469ms/step - loss: 0.1203 - accuracy: 0.9501\n",
            "Epoch 26/40\n",
            "955/955 [==============================] - 448s 469ms/step - loss: 0.0979 - accuracy: 0.9594\n",
            "Epoch 27/40\n",
            "955/955 [==============================] - 448s 469ms/step - loss: 0.0792 - accuracy: 0.9678\n",
            "Epoch 28/40\n",
            "955/955 [==============================] - 448s 469ms/step - loss: 0.0669 - accuracy: 0.9729\n",
            "Epoch 29/40\n",
            "955/955 [==============================] - 448s 469ms/step - loss: 0.0589 - accuracy: 0.9762\n",
            "Epoch 30/40\n",
            "955/955 [==============================] - 448s 469ms/step - loss: 0.0526 - accuracy: 0.9785\n",
            "Epoch 31/40\n",
            "955/955 [==============================] - 448s 469ms/step - loss: 0.0483 - accuracy: 0.9799\n",
            "Epoch 32/40\n",
            "955/955 [==============================] - 448s 469ms/step - loss: 0.0455 - accuracy: 0.9806\n",
            "Epoch 33/40\n",
            "955/955 [==============================] - 448s 469ms/step - loss: 0.0445 - accuracy: 0.9806\n",
            "Epoch 34/40\n",
            "955/955 [==============================] - 450s 471ms/step - loss: 0.0431 - accuracy: 0.9808\n",
            "Epoch 35/40\n",
            "955/955 [==============================] - 450s 472ms/step - loss: 0.0413 - accuracy: 0.9812\n",
            "Epoch 36/40\n",
            "955/955 [==============================] - 449s 470ms/step - loss: 0.0396 - accuracy: 0.9818\n",
            "Epoch 37/40\n",
            "955/955 [==============================] - 450s 471ms/step - loss: 0.0389 - accuracy: 0.9818\n",
            "Epoch 38/40\n",
            "955/955 [==============================] - 453s 474ms/step - loss: 0.0385 - accuracy: 0.9815\n",
            "Epoch 39/40\n",
            "955/955 [==============================] - 450s 472ms/step - loss: 0.0373 - accuracy: 0.9820\n",
            "Epoch 40/40\n",
            "955/955 [==============================] - 450s 471ms/step - loss: 0.0362 - accuracy: 0.9823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibWscsUBoC6K"
      },
      "source": [
        "# save checkpoint\n",
        "if True:\n",
        "#     checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    manager.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gewYrJ36oC6K"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "if True:\n",
        "#     checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "    checkpoint.restore(manager.latest_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCdRBRXPoC6L"
      },
      "source": [
        "### Функция для инференса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-yEgH_GoC6L",
        "outputId": "eb3143cd-d5f3-4184-b6e6-f9ba16e256e5"
      },
      "source": [
        "max_enc_seq_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0EiGH-IoC6L"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_dec_seq_length, max_enc_seq_length))\n",
        "    \n",
        "    input_seq = re.sub(r'[\\n]', '', sentence) # delete \\n as it's not in vocab\n",
        "    input_seq = prepare_string(input_seq).split(' ')\n",
        "    \n",
        "    # take only last N tokens, where N is max_seq_length for encoder input\n",
        "    if len(input_seq) > max_enc_seq_length:\n",
        "        input_seq = input_seq[-max_enc_seq_length:]\n",
        "        \n",
        "    input_seq = np.array([[input_token2idx.get(token, input_token2idx['<UNK>']) for token in input_seq]])\n",
        "    input_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                          input_seq,\n",
        "                          value=input_token2idx['<PAD>'],\n",
        "                          padding='post',\n",
        "                          maxlen=max_enc_seq_length)   \n",
        "\n",
        "    result = ''\n",
        "\n",
        "    # hidden = [tf.zeros((1, units))]\n",
        "    hidden = encoder.initialize_hidden_state(batch_sz=1) \n",
        "    enc_out, enc_hidden = encoder(input_seq, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_token2idx['<START>']], 0)\n",
        "    # dec_input = tf.convert_to_tensor([[target_token2idx['<START>']]]) # equivalent option\n",
        "\n",
        "    for t in range(max_dec_seq_length): # max_dec_seq_length or any other max length of output you want\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                            dec_hidden,\n",
        "                                                            enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "#         predicted_id = tf.argmax(predictions[0]).numpy() # max_prob prediction\n",
        "        temperature = 1.0\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1][0].numpy()\n",
        "\n",
        "        if target_idx2token[predicted_id] == '<END>':\n",
        "            break\n",
        "#             return result #, sentence, attention_plot\n",
        "\n",
        "        result += target_idx2token[predicted_id] + ' '\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    result = re.sub(r\"\\s([?.!,-])\", r\"\\1\", result).strip() # remove spaces before puctuation\n",
        "    \n",
        "    return result #, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW707Cx8oC6L",
        "outputId": "651fc9a4-f504-47b3-e591-9374e564fc5d"
      },
      "source": [
        "# some test sentences\n",
        "print(evaluate(\"Hey. Let's talk... Answer me!\"))\n",
        "print(evaluate(\"My name is John. What is your name?\"))\n",
        "print(evaluate(\"Hey. Let's talk\"))\n",
        "print(evaluate(\"It's a nice day.\"))\n",
        "print(evaluate(\"How old are you?\"))\n",
        "print(evaluate(\"Hi\"))\n",
        "print(evaluate(\"What's up?\"))\n",
        "print(evaluate(\"is he with you? are you travelling together? yes.\"))\n",
        "print(evaluate(\"i remember.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "poor guy's a bad guy...\n",
            "viceroy of the west indies.\n",
            "uh- huh. is that you had to get into something else?\n",
            "yeah... no way, then.\n",
            "eighteen.\n",
            "alone-- bill!\n",
            "what's it look like?\n",
            "please join us. come on forward.\n",
            "you'll be there, won't you.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK-0FhhHoC6L",
        "outputId": "39f849b5-876c-4a0f-b4cd-113085d7e4f9"
      },
      "source": [
        "print(evaluate(\"Hi, Elena. How are you?\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jogging. i was just sitting up for a minute.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FaVHeQuoC6L"
      },
      "source": [
        "after 40 epochs:\n",
        "\n",
        "User: what are you doing?<br>\n",
        "Bot1: are you my friend?<br>\n",
        "Bot2: yes.<br>\n",
        "Bot1: i am your friend.<br>\n",
        "Bot2: you ain't nice.<br>\n",
        "Bot1: maybe you wanted to fuck her. tomorrow friend.<br>\n",
        "Bot2: sir?<br>\n",
        "Bot1: just realized that, huh?<br>\n",
        "Bot2: realize you got it to me.\n",
        "\n",
        "User: How was your day, sir?<br>\n",
        "Bot1: complicated. i couldn't stop her apologizing.<br>\n",
        "Bot2: last year ago.<br>\n",
        "Bot1: anyone got any family?<br>\n",
        "Bot2: yes, he was.<br>\n",
        "Bot1: mm- mm... my wife...<br>\n",
        "Bot2: ... his majesty...<br>\n",
        "Bot1: well, let's hit that party.<br>\n",
        "Bot2: yes, but i think we should.\n",
        "\n",
        "User: Hi. do you want to talk?<br>\n",
        "Bot1: no.<br>\n",
        "User: Bye<br>\n",
        "Bot1: wanna go to dinner?<br>\n",
        "User: why not. Let's do it!<br>\n",
        "Bot1: thank god.<br>\n",
        "User: You are welcome. Ha<br>\n",
        "Bot1: daddy says i'm acting bad.<br>\n",
        "User: I guess so.<br>\n",
        "Bot1: but how did you do that?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXLF3h_BoC6M",
        "outputId": "8a74b207-9b39-45ca-af80-c943a3e3a5d5"
      },
      "source": [
        "# two bots talking...\n",
        "\n",
        "dialog = [\"How was your day, sir?\"]\n",
        "print(f'User: {dialog[0]}')\n",
        "for seq_index in range(0, 4):\n",
        "    length = len(dialog)\n",
        "    \n",
        "    decoded_sentence = evaluate(\" \".join(dialog[-min(4, length):]))\n",
        "    print(f'Bot1: {decoded_sentence}')\n",
        "    dialog.append(decoded_sentence)\n",
        "    length += 1\n",
        "    \n",
        "    decoded_sentence = evaluate(\" \".join(dialog[-min(4, length):]))\n",
        "    print(f'Bot2: {decoded_sentence}')\n",
        "    dialog.append(decoded_sentence)\n",
        "    length += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User: How was your day, sir?\n",
            "Bot1: complicated. i couldn't stop her apologizing.\n",
            "Bot2: things have on the walls before she died.\n",
            "Bot1: only common is all there.\n",
            "Bot2: his secretary is saving the trip.\n",
            "Bot1: just a minute. a few times i don't.\n",
            "Bot2: just go to the old lady's over.\n",
            "Bot1: i told ya i won't.\n",
            "Bot2: well, look--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoSnIXQEoC6M"
      },
      "source": [
        "#### Сохраняем веса всех слоев"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PE2hPXSoC6M",
        "outputId": "c7e86110-8b58-40b5-e57c-a24485f8a0dd"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/kigor/Documents/DataScience\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_c-2KUUoC6M"
      },
      "source": [
        "base_dir = '/home/kigor/Documents/DataScience/TF_Serving/exports/'\n",
        "enc_dir = base_dir + 'encoder/'\n",
        "dec_dir = base_dir + 'decoder/'\n",
        "vocab_dir = base_dir + 'vocabulary/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSZS9xojoC6M"
      },
      "source": [
        "Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlI7LFazoC6M",
        "outputId": "fab216a4-9563-4e19-b439-ed7ac65ea512"
      },
      "source": [
        "encoder.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.core.Masking at 0x7f2ee9607fa0>,\n",
              " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f2ee96078e0>,\n",
              " <tensorflow.python.keras.layers.wrappers.Bidirectional at 0x7f2ee96075e0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL_pCJAAoC6N",
        "outputId": "df70cc1f-bb12-4ba5-8489-3e5611c750bd"
      },
      "source": [
        "enc_embed_w = encoder.layers[1].get_weights()\n",
        "np.save(enc_dir + 'encoder_embed.npy', enc_embed_w[0])\n",
        "print(enc_embed_w[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17113, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FvJ1gOjoC6N",
        "outputId": "12123598-03b0-433c-eb16-f965d97fd1cf"
      },
      "source": [
        "enc_lstm_w = encoder.layers[2].get_weights()\n",
        "len(enc_lstm_w)\n",
        "for i, weights in enumerate(enc_lstm_w):\n",
        "    np.save(enc_dir + f'encoder_lstm_{i}.npy', weights)\n",
        "    print(f'{i}: {weights.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: (256, 8192)\n",
            "1: (2048, 8192)\n",
            "2: (8192,)\n",
            "3: (256, 8192)\n",
            "4: (2048, 8192)\n",
            "5: (8192,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thmqCRvmoC6N"
      },
      "source": [
        "Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhEWh3zAoC6N",
        "outputId": "d68ad509-ef98-4d0d-c1b4-b7d52c5aed28"
      },
      "source": [
        "decoder.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.core.Masking at 0x7f2ee9adbb80>,\n",
              " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f2ee9a7b3d0>,\n",
              " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f2ee9a7bb50>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7f2ee9a7bfd0>,\n",
              " <__main__.BahdanauAttention at 0x7f2ee96febb0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpIcBH53oC6N",
        "outputId": "e2c1ee20-8709-4bcf-b3cf-b98e1a604d4e"
      },
      "source": [
        "dec_embed_w = decoder.layers[1].get_weights()\n",
        "np.save(dec_dir + 'decoder_embed.npy', dec_embed_w[0])\n",
        "print(dec_embed_w[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17239, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekny4nEGoC6N",
        "outputId": "8433a735-20af-406d-835c-cb8fddfddb32"
      },
      "source": [
        "dec_lstm_w = decoder.layers[2].get_weights()\n",
        "for i, weights in enumerate(dec_lstm_w):\n",
        "    np.save(dec_dir + f'decoder_lstm_{i}.npy', weights)\n",
        "    print(f'{i}: {weights.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: (2304, 8192)\n",
            "1: (2048, 8192)\n",
            "2: (8192,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PChABKBroC6O",
        "outputId": "16bda4f3-db85-4a70-e997-9f2bad64225a"
      },
      "source": [
        "dec_fc_w = decoder.layers[3].get_weights()\n",
        "for i, weights in enumerate(dec_fc_w):\n",
        "    np.save(dec_dir + f'decoder_fc_{i}.npy', weights)\n",
        "    print(f'{i}: {weights.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: (2048, 17239)\n",
            "1: (17239,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voCbph6CoC6O",
        "outputId": "17391aa3-6d2a-4812-c5bb-417448c0f360"
      },
      "source": [
        "att_w1 = decoder.attention.W1.get_weights()\n",
        "for i, weights in enumerate(att_w1):\n",
        "    np.save(dec_dir + f'decoder_attW1_{i}.npy', weights)\n",
        "    print(f'{i}: {weights.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: (2048, 2048)\n",
            "1: (2048,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh-2PpjHoC6O",
        "outputId": "c512ffe3-b916-4fdd-ebd8-ff0b0bfd404e"
      },
      "source": [
        "att_w2 = decoder.attention.W2.get_weights()\n",
        "for i, weights in enumerate(att_w2):\n",
        "    np.save(dec_dir + f'decoder_attW2_{i}.npy', weights)\n",
        "    print(f'{i}: {weights.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: (2048, 2048)\n",
            "1: (2048,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYfPmEbeoC6O",
        "outputId": "a8738fa8-1adf-441c-b8a0-1b0d26ae458b"
      },
      "source": [
        "att_v = decoder.attention.V.get_weights()\n",
        "for i, weights in enumerate(att_v):\n",
        "    np.save(dec_dir + f'decoder_attV_{i}.npy', weights)\n",
        "    print(f'{i}: {weights.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: (2048, 1)\n",
            "1: (1,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn9TXyUvoC6O"
      },
      "source": [
        "Vocaburaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuSvdjuwoC6O"
      },
      "source": [
        "vocabs = [input_token2idx, target_token2idx, target_idx2token]\n",
        "names = ['input_token2idx', 'target_token2idx', 'target_idx2token']\n",
        "for name, vocab in zip(names, vocabs):\n",
        "    np.save(vocab_dir + name + '.npy', vocab)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}