{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1_Gym_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsolodkyi/NeuralNetworks_SkillBox/blob/main/module_16/1_Gym_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgshItiNbYmH"
      },
      "source": [
        "# Знакомство с Gym\n",
        "\n",
        "В этом уроке мы познакомимся с удобной средой для проведения RL экспериментов под названием Gym (https://gym.openai.com/). По аналогии с тем, как для обучения с учителем нам надо где-то брать обучающие датасеты, для обучения с подкреплением нам нужно иметь среду с определёнными правилами (или симуляцию это среды), в которой действует агент. \n",
        "\n",
        "в Gym содержатся реализации различных игр и других симуляций, на которых можно проводить эксперименты в области RL. Кроме самих симуляций в Gym имеется удобная обёртка для доступа к симуляции: мы сразу можем оперировтаь в терминах RL (состояние, действие, итд)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xiw_YVyvpeiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1c9g1MHeUCl"
      },
      "source": [
        "### Загрузка библиотек\n",
        "\n",
        "Загружаем библиотеку gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-zMowjHFz3t"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAGOjhbmg409"
      },
      "source": [
        "### Создание игровой среды\n",
        "\n",
        "С помощью функции `gym.make` мы можем создать симулятор необходимой нам игры. Рассмотрим пример игры `Frozen Lake`. Здесь есть ледяное поле и ямы (holes). Цель игры дойти до целевой позиции, не упав в яму. У этой игры есть парамтер `is_slippery`, который означает, \"будет ли лёд скользким\" -- будет ли среда всегда со стопроцентной вероятностью подчиняться нашему действию. Для простоты отключим этот флаг (куда захотели пойти, там и оказались).\n",
        "\n",
        "Другие доступные симуляции: https://gym.openai.com/envs/\n",
        "\n",
        "Посмотрим, сколько есть возможных действий и состояний в нашей игре. Кол-во состояний 16, так как столько ячеек в поле (потенциальных позиций для агента). А действий 4 (4 направления)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIprlbAyJo18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f265ed7-3776-486f-8754-4c087129f464"
      },
      "source": [
        "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "\n",
        "NUM_STATES = env.observation_space.n\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "\n",
        "print('States: {}'.format(NUM_STATES))\n",
        "print('Actions: {}'.format(NUM_ACTIONS))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "States: 16\n",
            "Actions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJxgn6Uowdex"
      },
      "source": [
        "### Основные функции Gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI-7omyUwUqE"
      },
      "source": [
        "С помощью `env.reset()` можно перезапустить среду в исходное состояние (на начало эпизода). Эта функция также вернет начальное состояние.\n",
        "\n",
        "В нашем случае состояние это просто число -- индекс соответствующей ячейки, где находится робот."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcUWhhTwHLyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3097bd5-5e28-42ed-c125-463b3bd029b6"
      },
      "source": [
        " s = env.reset()\n",
        " print(s)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tfMM2sVw1fG"
      },
      "source": [
        "С помощью функции `env.render()` можно визуализировать текущее состояние среды. В Colab это не всегда можно сделать довольно просто (в случае сложных симуляций), но в случае Frozen Lake это просто напечатанный текст с нашим полем 4x4. S - start, F - frozen, H - hole, G - goal. Маркером указано положение робота."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LvlYZK7NMVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d88a4b7-9c46-4ec0-e63f-99c73441294a"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnFEuExKxmie"
      },
      "source": [
        "Действия тоже кодируруются соответствующим индексом.\n",
        "Можно, например, выбрать случайное действие с попомщью функции `env.action_space.sample()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxQjACvNHNul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccad51e-2b84-4164-d650-3fec9df65d7a"
      },
      "source": [
        "a = env.action_space.sample()\n",
        "print(a)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtt-jbKsxoWy"
      },
      "source": [
        "Чтобы совершить действие `a` нужно вызвать функцию `env.step(a)`. Эта функция вернет новое состояние (`s1`), в которое мы перешли, награду `r`, информацию о том, завершилась ли игра (`done`) и другую менее важную информацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD-3K5-xHP8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc4798e-0e66-47b7-84a8-0a7b05d18b0e"
      },
      "source": [
        "s1, r, done, _ = env.step(a)\n",
        "\n",
        "print('New state: ', s1)\n",
        "print('Reward: ', r)\n",
        "print('Done? ', done)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New state:  5\n",
            "Reward:  0\n",
            "Done?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHjB31w2yVxY"
      },
      "source": [
        "Посмотрим, как теперь выглядит текущее состояние."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hn3ubSqNhDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0700b1e-85bf-49f0-8ea9-72070135dd01"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEJkMNdoycst"
      },
      "source": [
        "### Запуск симуляции\n",
        "\n",
        "Теперь у нас есть все знания, чтобы сыграть целый игровой эпизод с помощью Gym. Сначала встаём в стартовую позицию `env.reset()`. Потом в цикле совершаем шаги и рисуем промежуточные состояние с помощью `env.render()`. На каждом шаге нам как-то надо выбрать действие. Выбор действия обернём в функцию `a = policy(s)`. В идеале мы должны руководстсоваться некой стратегией, но в этом простом примере будем выбирать случайные действия `a`. После выбора действия `a` делаем шаг `env.step(a)` (сообщаем среде наше желание сделать действие). Если после определённого шага среда вернула `done=True`, значит произошел конец эпизода (упали в яму или дошли до цели). Если мы дошли до цели, то на последнем шаге мы должны были получить ненулевую награду."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r82qS-BZHU5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811da6bf-cbb1-46a1-80ef-eee76a38e710"
      },
      "source": [
        "s = env.reset()\n",
        "def policy():\n",
        "    \n",
        "    # non случайная стратегия\n",
        "    \n",
        "    happy_path = [1,1,2,1,2,2]\n",
        "    for i in happy_path:\n",
        "      env.render()\n",
        "      \n",
        "      s, r, done, _ = env.step(i) \n",
        "      print('Reward = {}'.format(r))\n",
        "      if done:\n",
        "        env.render()\n",
        "        print('Final reward = {}'.format(r))\n",
        "        break\n",
        "        \n",
        "policy()\n",
        "env.close()"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "Reward = 0.0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "Reward = 1.0\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Final reward = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q1RuTqqzk9A"
      },
      "source": [
        "**[Задание 1]** Запрограммируйте бота вручную на совершение таких действий, при которых он дойдёт до цели и получит ненулевую награду. Для этого измените лишь функцию `policy()`. Просто создайте набор правил (стратегию) -- какое действие надо совершить в зависимости от состояния. Проведите симуляцию с использованием вашей стратегии и посмотрите на результат."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#env.render()\n",
        "actions = {\n",
        "    'Left': 0,\n",
        "    'Down': 1,\n",
        "    'Right': 2, \n",
        "    'Up': 3\n",
        "}\n",
        " \n",
        "print('---- winning sequence ------ ')\n",
        "winning_sequence = (2 * ['Right']) + (3 * ['Down']) + ['Right']\n",
        "print(winning_sequence)\n",
        " \n",
        "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "env.reset()\n",
        "env.render()\n",
        " \n",
        "for a in winning_sequence:\n",
        "    new_state, reward, done, info = env.step(actions[a])\n",
        "    print()\n",
        "    env.render()\n",
        "    print(\"Reward: {:.2f}\".format(reward))\n",
        "    print(info)\n",
        "    if done:\n",
        "        break  \n",
        " \n",
        "print()"
      ],
      "metadata": {
        "id": "T-ALE4qloON8",
        "outputId": "4cbf4a6c-1ff9-47bd-dea1-c0ffc9dbdbe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- winning sequence ------ \n",
            "['Right', 'Right', 'Down', 'Down', 'Down', 'Right']\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "Reward: 0.00\n",
            "{'prob': 1.0}\n",
            "\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Reward: 1.00\n",
            "{'prob': 1.0}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "id": "F3sMxLIqq-vA",
        "outputId": "93d0b4c5-f6be-4308-c15c-992e8bcc7601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "id": "dLDxiFWmpf_m",
        "outputId": "5dc0ab9e-dc22-4112-aa82-cb9ddfbffc85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(2)\n"
      ],
      "metadata": {
        "id": "u-xuJlBJqWhm",
        "outputId": "494b0b68-bc5b-4b2c-ec13-7a02e47ca25c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 1.0, True, {'prob': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "id": "G7hYByE0sBJz",
        "outputId": "ac2b0c73-0ac0-46a9-ea38-67bbca6df72c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actions['Left']"
      ],
      "metadata": {
        "id": "F0sQXiYNreix",
        "outputId": "84b290ae-77b2-4289-a7f9-23abedc04136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}