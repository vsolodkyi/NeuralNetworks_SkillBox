{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "mid_ml_nlp_les-4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsolodkyi/NeuralNetworks_SkillBox/blob/main/module_13/mid_ml_nlp_les_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB1lU55Dqd6z"
      },
      "source": [
        "# Урок 4. Векторизация слов: Word2Vec\n",
        "\n",
        "Это более новый алгоритм, чем BoW.\n",
        "\n",
        "Алгоритм предполагает, что мы  уже разбили документы на токены и готовы скормить наши токены нейросети, которая сделает из каждого токена плотный вектор-эмбеддинг (см. ниже). И в этом уроке мы разберемся с тем, как у нейросети это получится."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL7CxG-Oqd63"
      },
      "source": [
        "При подходе BoW мы представляем каждый документ в виде разреженного вектора-строки, где размерность вектора соответствует количеству токенов в словаре. \n",
        "\n",
        "Нетрудно заметить, что при таком подходе игнорируется контекст, в котором находится слово. Например, в двух предложениях \"король издал указ\" и  \"правитель издал указ\" слова *король* и *правитель* являются синонимами, потому что используются в одинаковом контекста. Подход *BoW* не сможет уловить отношение синонимии.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_USCWWtqd64"
      },
      "source": [
        "\n",
        "Эту проблему решает подход `Word Embedding`, при котором каждое слово представляет собой вектор большой размерности (обычно несколько сотен). В отличие от подхода BoW, при котором каждое слово представляет собой разреженный вектор, *word embedding* - это \"плотный\" вектор. Классическим алгоритмом, вычисляющим эмбеддинги (то есть \"плотные\" вектора) слов, является Word2Vec, предложена чешским аспирантом Томашем Миколовым в 2013 году. Эта модель позволяет формировать векторы, которые отражают взаимоотношения между словами: \"король\" относится к \"королеве\" так же как \"женщина\" к \"мужчине\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn6ZBueyqd66"
      },
      "source": [
        "![word_vectors](https://sun9-40.userapi.com/c855436/v855436722/1f3fe6/o5y3vMysSbA.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGuHsY45qd67"
      },
      "source": [
        "Подход Word2Vec основан на интуитивно понятной гипотезе, которая называется гипотезой локальности — \"слова, которые встречаются в одинаковых окружениях, имеют близкие значения\". Эта гипотеза приводит к двум способам тренировки моделей: *Continious Bag of Words* (когда по контексту предсказываем слово) и *Skip Gram* - когда по слову пытаемся предсказать его контекст. Эмбеддинги, полученные с помощью обоих подходов оказываются идентичными - можно применять любой из них."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZv-f-7gqd69"
      },
      "source": [
        "Пример контекста: \n",
        "\n",
        "*Машинное обучение это* **класс** *методов искусственного интеллекта*\n",
        "\n",
        "Мы видим, что из текста вырезается окно текста, слово в центре окна мы хотим предсказать, используя слова по краям \"окна\" (тот самый *контекст*).\n",
        "\n",
        "На схеме представлены оба подхода:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nYGR_3tqd6-"
      },
      "source": [
        "![word2vec](https://sun9-43.userapi.com/c200816/v200816722/5fb72/o1o8GYCYul0.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EwsJuZAqd6_"
      },
      "source": [
        "На картинке представлен алгоритм тренировки *W2V*:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJMGOeeAqd7A"
      },
      "source": [
        "![w2v_net](https://sun9-59.userapi.com/c200816/v200816722/5fb7a/mX1xAY2vVgU.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwGEwvnRqd7B"
      },
      "source": [
        "На схеме слева-направо:\n",
        "\n",
        "* Входной вектор $(x_1,\\ldots,x_v)$ - слово из словаря, закодированное One-Hot\n",
        "* $W_{V\\times N}$ - матрица *word input* -  это эмбеддинги, которые мы обучаем\n",
        "* Эмбеддинг слова контекста $(h_1,\\ldots,h_N)$\n",
        "* $W`_{N\\times V}$ - матрица *word output* -  это тоже эмбеддинги но уже другие (они тоже обучаются в процессе)\n",
        "* Выходной вектор $(y_1,\\ldots,y_V)$ - скор для каждого слова из словаря размерности $V$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vF5fpMLqd7C"
      },
      "source": [
        "Мы видим два матричных перемножения - на самом деле W2V представляет собой очень простую нейронную сеть прямого распространения, *feed forward*. \n",
        "\n",
        "На схеме видны две матрицы-скрытые слои. На самом деле это эмбеддинги контента, которые мы обучаем, каждая строка - эмбеддинг размерности N. Матрица эмбеддингов размером (ЧИСЛО СЛОВ В СЛОВАРЕ) X (РАЗМЕРНОСТЬ ЭМБЕДДИНГА) в начале обучения инициализируется рандомными числами, которые “превращаются” в осмысленные эмбеддинги, пока сеть обучается методом обратного распространения ошибки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt41FyCwqd7D"
      },
      "source": [
        "На последнем слое мы получаем скоры для каждого слова из словаря. Скор (от англ score) с индексом i - это “уверенность” сети в том, что слово i может быть в контексте слова, которое мы прокидываем через сеть. То есть мы “кормим” сеть контекстом и уменьшаем лосс в случае, когда по контексту правильно удалось распознать слово внутри контекста. Слово с максимальным скором - это предсказание нашей сети. Зная \"истинное\" слово, которое мы предсказываем и то, что предсказала сеть, мы будем \"подкручивать\" веса эмбеддингов таким образом, чтобы лосс уменьшался и начинаем все лучше предсказывать слово по контексту."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1hCEW7dqd7G"
      },
      "source": [
        "Ниже показано, как работает, модификация *CBOW* - через нашу \"сеть\" пропускается каждое слово из контекста, мы пытаемся спрогнозировать слово \"внутри\" контекста:\n",
        "\n",
        "![cbow](https://sun9-65.userapi.com/c200816/v200816722/5fb81/oPfs4uHxw6I.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2_Gf4qjqd7I"
      },
      "source": [
        "В питоне существует модуль `gensim` который включает в себя библиотеки для обучения W2V. Этот модуль доступен для установки в [пакете Anaconda](https://anaconda.org/anaconda/gensim)\n",
        "\n",
        "Давайте применим алгоритм CBOW к нашему тексту:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU5_YjHEqd7K",
        "outputId": "f13294aa-7340-49b5-d07c-897b36e1007e"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "texts = df.tokenized.values\n",
        "\n",
        "model = Word2Vec(texts, size=10, window=7, min_count=2, workers=4, iter=10, sg=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-05 18:36:26,744 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
            "2019-05-05 18:36:26,745 : INFO : collecting all words and their counts\n",
            "2019-05-05 18:36:26,745 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-05-05 18:36:26,759 : INFO : collected 7968 word types from a corpus of 55882 raw words and 4499 sentences\n",
            "2019-05-05 18:36:26,760 : INFO : Loading a fresh vocabulary\n",
            "2019-05-05 18:36:26,768 : INFO : effective_min_count=2 retains 3452 unique words (43% of original 7968, drops 4516)\n",
            "2019-05-05 18:36:26,769 : INFO : effective_min_count=2 leaves 51366 word corpus (91% of original 55882, drops 4516)\n",
            "2019-05-05 18:36:26,778 : INFO : deleting the raw counts dictionary of 7968 items\n",
            "2019-05-05 18:36:26,779 : INFO : sample=0.001 downsamples 39 most-common words\n",
            "2019-05-05 18:36:26,779 : INFO : downsampling leaves estimated 35698 word corpus (69.5% of prior 51366)\n",
            "2019-05-05 18:36:26,786 : INFO : estimated required memory for 3452 words and 10 dimensions: 2002160 bytes\n",
            "2019-05-05 18:36:26,786 : INFO : resetting layer weights\n",
            "2019-05-05 18:36:26,816 : INFO : training model with 4 workers on 3452 vocabulary and 10 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n",
            "2019-05-05 18:36:26,839 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:26,840 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:26,840 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:26,842 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:26,843 : INFO : EPOCH - 1 : training on 55882 raw words (35663 effective words) took 0.0s, 1692618 effective words/s\n",
            "2019-05-05 18:36:26,866 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:26,866 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:26,867 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:26,869 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:26,870 : INFO : EPOCH - 2 : training on 55882 raw words (35706 effective words) took 0.0s, 1610012 effective words/s\n",
            "2019-05-05 18:36:26,891 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:26,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:26,895 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:26,896 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:26,896 : INFO : EPOCH - 3 : training on 55882 raw words (35733 effective words) took 0.0s, 1636836 effective words/s\n",
            "2019-05-05 18:36:26,916 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:26,918 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:26,919 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:26,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:26,921 : INFO : EPOCH - 4 : training on 55882 raw words (35595 effective words) took 0.0s, 1815376 effective words/s\n",
            "2019-05-05 18:36:26,941 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:26,946 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:26,947 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:26,948 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:26,948 : INFO : EPOCH - 5 : training on 55882 raw words (35720 effective words) took 0.0s, 1605076 effective words/s\n",
            "2019-05-05 18:36:26,971 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:26,972 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:26,973 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:26,976 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:26,977 : INFO : EPOCH - 6 : training on 55882 raw words (35770 effective words) took 0.0s, 1456745 effective words/s\n",
            "2019-05-05 18:36:27,000 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:27,001 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:27,004 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:27,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:27,006 : INFO : EPOCH - 7 : training on 55882 raw words (35732 effective words) took 0.0s, 1514581 effective words/s\n",
            "2019-05-05 18:36:27,027 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:27,028 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:27,029 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:27,031 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:27,031 : INFO : EPOCH - 8 : training on 55882 raw words (35592 effective words) took 0.0s, 1724412 effective words/s\n",
            "2019-05-05 18:36:27,054 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:27,055 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:27,056 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:27,057 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:27,057 : INFO : EPOCH - 9 : training on 55882 raw words (35747 effective words) took 0.0s, 1619578 effective words/s\n",
            "2019-05-05 18:36:27,079 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-05-05 18:36:27,080 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-05-05 18:36:27,081 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-05-05 18:36:27,082 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-05-05 18:36:27,083 : INFO : EPOCH - 10 : training on 55882 raw words (35646 effective words) took 0.0s, 1718373 effective words/s\n",
            "2019-05-05 18:36:27,083 : INFO : training on a 558820 raw words (356904 effective words) took 0.3s, 1340699 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsGIKNuXqd7T"
      },
      "source": [
        "Мы обучили эмбеддинги слов. Давайте проверим, какой вектор обучился для слова `android`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oBhECavqd7U",
        "outputId": "fd21e9df-6c90-4bb5-ba65-926613c4613d"
      },
      "source": [
        "model.wv.get_vector('android')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.26981112,  1.2430197 , -3.6010602 ,  0.98848987,  0.6809826 ,\n",
              "        2.8977017 , -0.6469272 , -0.03298385,  0.0875835 , -0.9362278 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6OxrBFDqd7Y"
      },
      "source": [
        "Мы видим набор цифр - это вектор длины 10. Давайте найдём, какие слова соответствуют максимально похожим векторам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7OmdXRHqd7a",
        "outputId": "ab1dd298-1332-43b9-f374-682c21d262d5"
      },
      "source": [
        "model.wv.most_similar('android')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('blackberry', 0.9917972087860107),\n",
              " ('iphone', 0.9915081858634949),\n",
              " ('updates', 0.9836416840553284),\n",
              " ('shoes', 0.9720171689987183),\n",
              " ('condensed', 0.9703693985939026),\n",
              " ('christian', 0.968561053276062),\n",
              " ('angrybirds', 0.9650596976280212),\n",
              " ('whiteboarding', 0.9633722901344299),\n",
              " ('yo', 0.9629340767860413),\n",
              " ('grill', 0.9623512029647827)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzBMBY_Sqd7e"
      },
      "source": [
        "Мы видим, что модель обучила похожие вектора для слов `blackberry`, `iphone`  - это всё названия телефонов, то есть модель работает!\n",
        "\n",
        "На основе векторизованных слов можно строить векторное описание целого предложения - такой алгоритм называется `doc2vec`. "
      ]
    }
  ]
}