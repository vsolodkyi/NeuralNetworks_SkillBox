{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsolodkyi/NeuralNetworks_SkillBox/blob/main/module_2/homework_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRRejAlppFwP"
      },
      "source": [
        "# Обучение нейронной сети\n",
        "\n",
        "Это последняя и самая важная практика в этом блоке. В ней вы соберете воедино все, что мы с вами изучили и примените для создания сети, которая классифицирует рукописные цифры.\n",
        "\n",
        "Задание будет состоять из следующих этапов:\n",
        "1.  Реализация слоя ReLU\n",
        "2.  Реализация полносвязного слоя\n",
        "3.  Написание обучающего цикла\n",
        "4.  Загрузка данных и обучение сети\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZpTEvkPqxqk"
      },
      "source": [
        "**Задание 1**\n",
        "Написание слоя ReLU. В этом задании вы реализуете слой нейронной сети, который вычисляет поэлементно функцию ReLU:\n",
        "\n",
        "$$\n",
        "ReLU(x) = max(0, x)\n",
        "$$\n",
        "\n",
        "и выгядит следующим образом:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/536/1*oePAhrm74RNnNEolprmTaQ.png)\n",
        "\n",
        "Нетрудно заметить, что производная при $x > 0$ равна $1$, а при $x < 0$ равна $0$. Это вам пригодится при реализации backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKoIb3NpLAqj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.misc import derivative\n",
        "from typing import List\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from IPython.display import clear_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fiEdDgzLFiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "9ce90d56-4f98-4392-fbb3-96edb9d50071"
      },
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    Базовый класс слоя нашей нейронной сети. \n",
        "    Все слои должны наследоваться от него и реализовывать два метода: forward и backward\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "    def backward(self, dL_dz, learning_rate=0):\n",
        "        pass\n",
        "\n",
        "class ReLU(Layer):\n",
        "    \"\"\"\n",
        "    Слой ReLU\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Метод, который вычисляет ReLU(x)\n",
        "\n",
        "        Размер выхода должен совпадать со входом\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self._saved_input = x # нам необходимо сохранить вход\n",
        "        output = None\n",
        "\n",
        "        # < YOUR CODE STARTS HERE >\n",
        "        # переменная output должна содержать выход ReLU\n",
        "\n",
        "        # подсказка: метод clip библиотеки numpy позволяет при заданном интервале значения вне интервала обрезать по краям интервала. \n",
        "        # Например, если указан интервал [0, 1], \n",
        "        # значения меньше 0 становятся 0, а значения больше 1 становятся 1.\n",
        "        \n",
        "        output = \n",
        "        \n",
        "        # < YOUR CODE ENDS HERE >\n",
        "        assert output.shape == x.shape\n",
        "        return output\n",
        "\n",
        "    def backward(self, dL_dz, learning_rate=0.):\n",
        "        \"\"\"\n",
        "        dL_dz -- производная финальной функции по выходу этого слоя.\n",
        "                 Размерость должна в точности соответствовать размерности\n",
        "                 x, который прошел в forward pass.\n",
        "        learning_rate -- не используется, т.к. ReLU не содержит параметров.\n",
        "\n",
        "        Метод должен посчитать производную dL_dx.\n",
        "        Благодаря chain rule, мы знаем, что dL_dx = dL_dz * dz_dx\n",
        "        и при этом dL_dz нам известна.\n",
        "\n",
        "        Для слоя relu, dz_dx(x) = 1, при x > 0, и dz_dz = 0 при x < 0\n",
        "        \n",
        "        * сохраненный инпут находится в self._saved_input\n",
        "        \"\"\"\n",
        "        dz_dx = None\n",
        "        \n",
        "        # < YOUR CODE STARTS HERE >\n",
        "        # переменная dz_dx должна содержать производную выхода ReLU по ее входу\n",
        "        \n",
        "        dz_dx = \n",
        "        \n",
        "        # < YOUR CODE ENDS HERE >\n",
        "        assert dz_dx.shape == self._saved_input.shape, f\"Shapes must be the same. Got {dz_dx.shape, self._saved_input.shape}\"\n",
        "        output = dz_dx * dL_dz\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-fa775763c863>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    output =\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWFBXJGPz_zt"
      },
      "source": [
        "relu = ReLU()\n",
        "\n",
        "# убедитесь, что график соответствует представленному вверху\n",
        "plt.plot(np.linspace(-1, 1, 100), relu.forward(np.linspace(-1, 1, 100)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH8pmNuI3BBP"
      },
      "source": [
        "f = lambda x: ReLU().forward(x)\n",
        "\n",
        "x = np.linspace(-1, 1, 10*32).reshape([10, 32])\n",
        "l = ReLU()\n",
        "l.forward(x)\n",
        "grads = l.backward(np.ones([10, 32]))\n",
        "numeric_grads = derivative(f, x, dx=1e-6)\n",
        "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
        "     \"gradient returned by your layer does not match the numerically computed gradient\"\n",
        "print(\"Test passed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddzfoKcogJfE"
      },
      "source": [
        "**Задание 2** Реализация полносвязного слоя. \n",
        "\n",
        "Закончите реализацию метода forward для полносвязного слоя"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5DQKmPuqKUp"
      },
      "source": [
        "class FCLayer(Layer):\n",
        "    \"\"\"\n",
        "    Полносвязный (fully connected/dense) слой\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        \"\"\"\n",
        "        in_dim, out_dim -- количество входных и выходных нейронов соответственно\n",
        "        \"\"\"\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        # инициализируем матрицу весов (in_dim,out_dim) нормальным распределением\n",
        "        self.weights = np.random.randn(in_dim,out_dim)*0.001\n",
        "\n",
        "        # инициализируем смещение нулями\n",
        "        self.bias = np.zeros(self.out_dim)\n",
        "        self._saved_input = None\n",
        "    \n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Вычисление выхода полносвязного слоя.\n",
        "\n",
        "        x -- вход слоя, размерности (N, in_dim), где N -- количество объектов \n",
        "             в батче\n",
        "\n",
        "        return: matmul(x, weights) + bias\n",
        "        \"\"\"\n",
        "        assert np.ndim(x) == 2\n",
        "        assert x.shape[1] == self.in_dim\n",
        "        self._saved_input = x\n",
        "\n",
        "        # < YOUR CODE STARTS HERE >\n",
        "        # переменная output должна содержать выход полносвязного слоя\n",
        "        \n",
        "        output = \n",
        "        \n",
        "        # < YOUR CODE ENDS HERE >\n",
        "\n",
        "        assert output.shape == (x.shape[0], self.out_dim), (output.shape, (x.shape[0], self.out_dim))\n",
        "        return output\n",
        "    \n",
        "    def backward(self, dL_dz, learning_rate=0.):\n",
        "        \"\"\"\n",
        "        dL_dz -- производная финальной функции по выходу этого слоя.\n",
        "                 Размерость (N, self.out_dim).\n",
        "        learning_rate -- если отличен от нуля, то с вызовом этой функции, параметры\n",
        "                         слоя (weights, bias) будут обновлены\n",
        "\n",
        "        Метод должен посчитать производную dL_dx.\n",
        "        \n",
        "        \"\"\"\n",
        "        assert np.ndim(dL_dz) == 2\n",
        "        assert dL_dz.shape[1] == self.out_dim\n",
        "        \n",
        "        # очень рекомендуем понять почему это так,\n",
        "        # хорошее объяснение здесь: http://cs231n.stanford.edu/handouts/linear-backprop.pdf\n",
        "        self.dL_dw = np.dot(self._saved_input.T, dL_dz)\n",
        "        self.dL_dx = np.dot(dL_dz, self.weights.T)\n",
        "        self.dL_db = dL_dz.sum(0) \n",
        "        \n",
        "        assert self.dL_db.shape == self.bias.shape\n",
        "        assert self.dL_dw.shape == self.weights.shape\n",
        "        assert self.dL_dx.shape == self._saved_input.shape\n",
        "\n",
        "        if learning_rate != 0:\n",
        "            # знакомый вам шаг градиентного спуска!\n",
        "            self.weights -= learning_rate * self.dL_dw\n",
        "            self.bias -= learning_rate * self.dL_db\n",
        "        \n",
        "        return self.dL_dx\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pREQ8CMW_Bcw"
      },
      "source": [
        "Теперь перейдем к вычислению лосса. Т.к. перед нами стоит задача мультиклассовой классификации, то мы будем использовать multiclass_cross_entropy_loss, который мы рассмотрели на лекции.\n",
        "Такой лосс на одном объекте $i$, который принадлежит к классу $k$ будет равен: \n",
        "$$\n",
        "loss_i = -log(p_i^k),\n",
        "$$\n",
        "где $p_i^k$ -- предсказанная вероятность того, что $i$ый объект принадлежит к классу $k$.\n",
        "\n",
        "Для того, чтобы получить вероятности, мы использовали Softmax, примененный в логитам выходного слоя. Т.е. \n",
        "$$\n",
        "p_i^k = \\frac{exp(logit_k)}{\\sum_{j=0}^{m} exp(logit_j)}\n",
        "$$\n",
        "\n",
        "А значит лосс можно переписать так:\n",
        "$$\n",
        "loss_i = -log(p_i^k) = -logit_k + log(\\sum_{j=0}^{m} exp(logit_j))\n",
        "$$\n",
        "\n",
        "Как мы помним, сеть изначально предсказывает логиты, и затем мы превращаем их в вероятности. Но т.к. мы знаем, что нам предстоит считать лосс, то мы можем не тратить \"силы\" на вычисление вероятностей и посчитать лосс основываясь на логитах. Такая запись проще и вычислительно более стабильная. \n",
        "\n",
        "Ниже мы приводим готовую реализацию лосса и его градиента."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5b5T9VG1ohv"
      },
      "source": [
        "def multiclass_crossentropy_with_logits(logits, y_true):\n",
        "    \"\"\"\n",
        "    logits -- выход нейронной сети без активации. Размерность: (N, k),\n",
        "              где N -- количество объектов, k -- количество классов\n",
        "    y_true -- реальные классы для N объектов\n",
        "\n",
        "    Класс возвращает вектор из лоссов на каждом объекте\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    logits_for_answers = logits[np.arange(len(logits)), y_true]\n",
        "    \n",
        "    cross_entropy = -logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "    \n",
        "    return cross_entropy\n",
        "\n",
        "def grad_multiclass_crossentropy_with_logits(logits, y_true):\n",
        "    \"\"\"\n",
        "     logits -- выход нейронной сети без активации. Размерность: (N, k),\n",
        "              где N -- количество объектов, k -- количество классов\n",
        "    y_true -- реальные классы для N объектов\n",
        "\n",
        "    Класс возвращает матрицу производных.\n",
        "    \n",
        "    \"\"\"\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)), y_true] = 1\n",
        "    \n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "    \n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH5c18IUgtj5"
      },
      "source": [
        "**Задание 3** Реализация класса  Network.\n",
        "\n",
        "В этом задании вам предлагается реализовать методы forward(), predict(), train_step(). Это последний шаг перед тем как наша сеть будет готова!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-H0nTyx1lTH"
      },
      "source": [
        "class Network:\n",
        "    \"\"\"\n",
        "    Нейронная сеть\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, layers: List[Layer]):\n",
        "        \"\"\"\n",
        "        Для инициализации нейронной сети, нам нужен список слоев, которые должны\n",
        "        быть последовательно применены друг к другу. \n",
        "\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "    \n",
        "    def forward(self, x: np.ndarray):\n",
        "        \"\"\"\n",
        "        x -- входной батч объектов размера (N, размер_входа_первого_слоя)\n",
        "        \n",
        "        Получив x на вход, сеть должна по-очереди применить к нему все слои.\n",
        "        Т.е. выход каждого слоя является входом следующего.\n",
        "\n",
        "        x -> layer_0 -> layer_1 ... -> last_layer\n",
        "\n",
        "        \"\"\"\n",
        "        output = None\n",
        "        # < YOUR CODE STARTS HERE >\n",
        "        # Реализуйте последовательное применение forward методов для каждого из \n",
        "        # слоев (self.layers)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        # < YOUR CODE ENDS HERE >\n",
        "        \n",
        "        return output\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        x -- входной батч объектов размера (N, размер_входа_первого_слоя)\n",
        "        \n",
        "        Функция должна вернуть вектор размера (N) с номером предсказанного класса\n",
        "        для каждого объекта. \n",
        "\n",
        "        \"\"\"\n",
        "        logits = self.forward(x) # считаем логиты сделав полный форвард пасс сети\n",
        "        # напомним, что размер логитов (N, k), где k -- количество классов\n",
        "        \n",
        "        classes = None\n",
        "        # < YOUR CODE STARTS HERE >\n",
        "        # реализуйте получение классов из логитов. вам может пригодится \n",
        "        # метод argmax\n",
        "\n",
        "        classes = \n",
        "\n",
        "        # < YOUR CODE ENDS HERE >\n",
        "\n",
        "        assert classes.shape == (x.shape[0],)\n",
        "        return classes\n",
        "    \n",
        "    def train_step(self, x, y, learning_rate):\n",
        "        \"\"\"\n",
        "        x -- входной батч объектов размера (N, размер_входа_первого_слоя)\n",
        "        y -- реальные классы объектов (N,)\n",
        "        \"\"\"\n",
        "        logits = self.forward(x)\n",
        "        loss = multiclass_crossentropy_with_logits(logits, y)\n",
        "        loss_grad = grad_multiclass_crossentropy_with_logits(logits, y)\n",
        "        \n",
        "        # < YOUR CODE STARTS HERE >\n",
        "        # Выше мы получили loss_grad. Теперь его нужно \"пробросить\" через всю сеть\n",
        "        # вызывая backward каждого слоя в обратном порядке.\n",
        "        # Не забудьте передать в backward -- learning rate.\n",
        "        # loss_grad -> last_layer.backward(loss_grad, lr) -> ... --> layer_0.backward(loss_grad_from_layer_1, lr)\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        # < YOUR CODE ENDS HERE >\n",
        "\n",
        "        return np.mean(loss)\n",
        "    \n",
        "    def fit(self, x_train, y_train, x_test, y_test, learning_rate, num_epochs, \n",
        "            batch_size):\n",
        "        \"\"\"\n",
        "        Цикл обучения уже реализован. Основная его задача -- итерироваться по \n",
        "        минибатчам и вызывать на каждом из них train_step, который вы уже реализовали.\n",
        "\n",
        "        В остальном -- это логирование лосса, точности и отрисовка графика.\n",
        "\n",
        "        \"\"\"\n",
        "        train_log = []\n",
        "        test_log = []\n",
        "        loss_log = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            loss_iters = []\n",
        "            for x_batch,y_batch in iterate_minibatches(x_train, y_train, \n",
        "                                                       batchsize=batch_size, shuffle=True):\n",
        "                loss_iters.append(self.train_step(x_batch, y_batch, learning_rate=learning_rate))\n",
        "\n",
        "            loss_log.append(np.mean(loss_iters)) # для визуализации усредняем лосс за каждую итерацию\n",
        "            train_accuracy = accuracy_score(y_train, self.predict(x_train))\n",
        "            test_accuracy = accuracy_score(y_test, self.predict(x_test))\n",
        "            train_log.append(train_accuracy)\n",
        "            test_log.append(test_accuracy)\n",
        "\n",
        "            clear_output()\n",
        "            print(\"Epoch\", epoch)\n",
        "            print(\"Train accuracy:\",train_log[-1])\n",
        "            print(\"Test accuracy:\",test_log[-1])\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            ax1 = plt.subplot(1,2,1)    \n",
        "            plt.plot(train_log,label='train accuracy')\n",
        "            plt.plot(test_log,label='test accuracy')\n",
        "            ax2 = plt.subplot(1,2,2)\n",
        "            plt.plot(loss_log,label='loss')\n",
        "            ax1.legend(loc='best')\n",
        "            ax2.legend(loc='best')\n",
        "            plt.grid()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in tqdm(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
        "        if shuffle:\n",
        "            batch_indexes = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            batch_indexes = slice(start_idx, start_idx + batchsize)\n",
        "            \n",
        "        yield inputs[batch_indexes], targets[batch_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QirWyEeEPS3w"
      },
      "source": [
        "## Обучение\n",
        "На этом этапе наша сеть полностью описаны и готова к бою. Нам нужны теперь только данные.\n",
        "\n",
        "Мы поставим перед собой задачу классифицировать изображения рукописных цифр. Они представляют собой картинки размера (28, 28). Для использования нашей сети мы превратим их в строчки длины $28*28 = 784$.\n",
        "\n",
        "Давайте посмотрим как они выглядят:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHeWIyY4fER3"
      },
      "source": [
        "def show_mnist(images, labels, predicted_labels=None):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(16):\n",
        "        plt.subplot(4,4, i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap=plt.cm.gray)\n",
        "        if predicted_labels is not None:\n",
        "            title_obj = plt.title(f\"Real: {labels[i]}. Pred: {predicted_labels[i]}\")\n",
        "            if labels[i] != predicted_labels[i]:\n",
        "                plt.setp(title_obj, color='r')\n",
        "        else:\n",
        "            plt.title(f\"Real label: {labels[i]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prp9Oo0mhNbf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "show_mnist(train_images, train_labels)\n",
        "\n",
        "# преобразуем изображения к нужному виду\n",
        "train_images = train_images.reshape(train_images.shape[0], -1).astype('float32') / 255.\n",
        "test_images = test_images.reshape(test_images.shape[0], -1).astype('float32') / 255.\n",
        "print(train_images.shape, test_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFTgCXa4QQZm"
      },
      "source": [
        "# Наша сеть будет состоять из одного скрытого слоя\n",
        "layers = []\n",
        "layers.append(FCLayer(train_images.shape[1], 100)) # входной слой\n",
        "layers.append(ReLU()) # активация\n",
        "layers.append(FCLayer(100, 200)) # скрытый слой\n",
        "layers.append(ReLU()) # активация\n",
        "layers.append(FCLayer(200, 10)) # выходной слой -- 10 классов (10 цифр). Обратите внимание, мы не используем активацию!\n",
        "                                # т.к. лосс рассчитывает на логиты, а не вероятности\n",
        "\n",
        "# инициализируем наш класс указанными слоями\n",
        "net = Network(layers=layers)\n",
        "# если все реализовано правильно -- точность на отложенной части выборки достигнет 97%\n",
        "net.fit(x_train=train_images, y_train=train_labels, \n",
        "        x_test=test_images, y_test=test_labels,\n",
        "        batch_size=32, num_epochs=10, learning_rate=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfOs96iPfJCj"
      },
      "source": [
        "predicted_labels = net.predict(test_images)\n",
        "idxs = np.random.choice(np.arange(len(test_images)), 16, replace=False)\n",
        "show_mnist(test_images[idxs].reshape((-1, 28, 28)), test_labels[idxs], predicted_labels[idxs])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}