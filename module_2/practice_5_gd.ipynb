{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_5_gd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsolodkyi/NeuralNetworks_SkillBox/blob/main/module_2/practice_5_gd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9BZlZjCI6Nw"
      },
      "source": [
        "# Градиентный спуск\n",
        "\n",
        "В этом практическом занятии вы реализуете градиентный спуск самостоятельно. На сегодняшний день -- это основной способ оптимизации нейронных сетей.\n",
        "\n",
        "Мы начнем с простой функции -- парабола, затем перейдем к чуть более сложной."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NdsVzyU6BvR"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jrE7v_ZQfK-"
      },
      "source": [
        "**Задание 1**\n",
        "\n",
        "Закончите реализацию алгоритма градиентного спуска. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cV_3uyWJXVB"
      },
      "source": [
        "def gradient_descent(f: callable, df_dx: callable, initial_position: float, n_iters: int, lr: float,\n",
        "                     tol: float = 1e-3):\n",
        "    \"\"\"\n",
        "    Алгоритм градиентного спуска. \n",
        "\n",
        "    f -- функция, минимум которой мы хотим найти\n",
        "    df_dx -- производная функции по x\n",
        "    initial_position -- начальное приближение\n",
        "    n_iters -- максимальное количество шагов градиентного спуска\n",
        "    lr -- learning rate, скорость обучения\n",
        "    tol -- точность с которой мы будем считать, что значение функции не меняется\n",
        "    \n",
        "    Функция должна вернуть найденный минимум, историю положений\n",
        "    и значений функции \n",
        "    \"\"\"\n",
        "    positions = []\n",
        "    values = []\n",
        "    position = initial_position\n",
        "    for i in range(n_iters):\n",
        "        positions.append(position)\n",
        "        values.append(f(position))\n",
        "        if len(values) > 1:\n",
        "            if np.abs(values[-1] - values[-2]) < tol:\n",
        "                break\n",
        "        \n",
        "        # < YOUR CODE STARTS HERE >\n",
        "        # Реализуйте шаг градиентного спуска. \n",
        "        position = \n",
        "        # < YOUR CODE ENDS HERE >\n",
        "\n",
        "    print(f\"Found minimum at x={position} after {len(positions) - 1} steps.\")\n",
        "    return position, positions, values\n",
        "\n",
        "def visualize(f: callable, limits: tuple, positions: list, values: list):\n",
        "    xs = np.linspace(limits[0], limits[1], 100)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(xs, f(xs))\n",
        "    plt.title(f\"Найденный минимум после {len(positions) - 1} шагов: $f = {values[-1]:.2}$ при $x = {positions[-1]:.2}$\")\n",
        "    _, = plt.plot(positions, values)\n",
        "    for p, v in zip(positions, values):\n",
        "        plt.scatter(p, v, s=30, zorder=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiHHXucn6f1T"
      },
      "source": [
        "f = lambda x: x**2 + x + 1 # функция для оптимизации\n",
        "df_dx = lambda x: 2*x + 1 # производная\n",
        "\n",
        "lr = 0.3\n",
        "n_iters = 20\n",
        "initial_position = -4.\n",
        "\n",
        "\n",
        "position, positions, values = gradient_descent(f=f, df_dx=df_dx, initial_position=initial_position, \n",
        "                                               n_iters=n_iters, lr=lr, tol=1e-4)\n",
        "\n",
        "visualize(f, (-4, 3), positions=positions, values=values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-nf4o4695AC",
        "cellView": "both"
      },
      "source": [
        "# градиентный спуск\n",
        "initial_position = -4 \n",
        "f = lambda x: (x**2 - 4*x + 4)*(x**2 + 4*x + 2)\n",
        "\n",
        "# < YOUR CODE STARTS HERE>\n",
        "# Напишите, чему равна производная f по x\n",
        "df_dx = \n",
        "# < YOUR CODE ENDS HERE>\n",
        "\n",
        "# < YOUR CODE STARTS HERE>\n",
        "# Выберите подходящие параметры для оптимизации\n",
        "n_iters =  \n",
        "lr = \n",
        "# < YOUR CODE ENDS HERE>\n",
        "\n",
        "\n",
        "position, positions, values = gradient_descent(f=f, df_dx=df_dx, initial_position=initial_position, \n",
        "                                               n_iters=n_iters, lr=lr, tol=1e-4)\n",
        "visualize(f, (-4, 4), positions=positions, values=values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz_U7XdgTtn4"
      },
      "source": [
        "**Задание 2**\n",
        "Продемонстрируйте на примере функции выше:\n",
        "1. застревание в локальном минимуме (выберете другое начальное приближение)\n",
        "2. расхождение оптимизации (попробуйте другую скорость обучения)\n",
        "\n",
        "Скопируйте код из клетки выше в две последующие и оставьте графики для оценки и демонстрации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMI3pff4THuR"
      },
      "source": [
        "# Задание 2.1\n",
        "\n",
        "# градиентный спуск\n",
        "initial_position = 3 \n",
        "f = lambda x: (x**2 - 4*x + 4)*(x**2 + 4*x + 2)\n",
        "\n",
        "# < YOUR CODE STARTS HERE>\n",
        "# Напишите, чему равна производная f по x\n",
        "df_dx = \n",
        "# < YOUR CODE ENDS HERE>\n",
        "\n",
        "# < YOUR CODE STARTS HERE>\n",
        "# Выберите подходящие параметры для оптимизации\n",
        "n_iters =  \n",
        "lr = \n",
        "# < YOUR CODE ENDS HERE>\n",
        "\n",
        "\n",
        "position, positions, values = gradient_descent(f=f, df_dx=df_dx, initial_position=initial_position, \n",
        "                                               n_iters=n_iters, lr=lr, tol=1e-4)\n",
        "visualize(f, (-4, 4), positions=positions, values=values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQWqtuzXTMuB"
      },
      "source": [
        "# Задание 2.1\n",
        "\n",
        "# градиентный спуск\n",
        "initial_position = 0\n",
        "f = lambda x: (x**2 - 4*x + 4)*(x**2 + 4*x + 2)\n",
        "\n",
        "# < YOUR CODE STARTS HERE>\n",
        "# Напишите, чему равна производная f по x\n",
        "df_dx = \n",
        "# < YOUR CODE ENDS HERE>\n",
        "\n",
        "# < YOUR CODE STARTS HERE>\n",
        "# Выберите подходящие параметры для оптимизации\n",
        "n_iters =  \n",
        "lr = \n",
        "# < YOUR CODE ENDS HERE>\n",
        "\n",
        "\n",
        "position, positions, values = gradient_descent(f=f, df_dx=df_dx, initial_position=initial_position, \n",
        "                                               n_iters=n_iters, lr=lr, tol=1e-4)\n",
        "visualize(f, (-4, 4), positions=positions, values=values)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}